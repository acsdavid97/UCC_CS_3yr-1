{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Reinforcement Learning</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Reinforcement learning</h1>\n",
    "<ul>\n",
    "    <li>The agent carries out an action</li>\n",
    "    <li>A teacher or the environment provides a <b>reward</b> (or punishment), often delayed, \n",
    "        that acts as positive (or negative) reinforcement\n",
    "        <ul>\n",
    "            <li>making it more (or less) likely that the agent will execute that action if it find itself in the same\n",
    "                or similar situation in the future\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>For simplicity, in this lecture, we assume a fully-observable, deterministic environment</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Reward</h1>\n",
    "<figure>\n",
    "    <img src=\"images\\04_reward.png\" />\n",
    "</figure>\n",
    "<p style=\"font-size: 2em\">\n",
    "    $$\\v{s}_0 \\xrightarrow[\\,\\,\\,\\,\\,\\,\\,\\,r_0]{a_0\\,\\,\\,\\,\\,\\,\\,\\,} \\v{s}_1 \\xrightarrow[\\,\\,\\,\\,\\,\\,\\,\\,r_1]{a_1\\,\\,\\,\\,\\,\\,\\,\\,} \\v{s}_2 \\xrightarrow[\\,\\,\\,\\,\\,\\,\\,\\,r_2]{a_2\\,\\,\\,\\,\\,\\,\\,\\,} \\cdots$$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Cumulative reward</h1>\n",
    "<ul>\n",
    "    <li>Cumulative reward: \n",
    "        $$r_0 + \\gamma r_1 + \\gamma^2r_2 + \\cdots$$    \n",
    "        or\n",
    "        $$\\sum_{t=0}^{t=\\infty}\\gamma^tr_t$$\n",
    "        where $\\gamma$ is the <b>discount rate</b> ($0 \\leq \\gamma \\leq 1$)\n",
    "    </li>\n",
    "    <li>The task of the agent is to learn an action function that maximises cumulative reward</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Action-value function</h1>\n",
    "<ul>\n",
    "    <li>Assume 2 Boolean sensors and 3 actions</li>\n",
    "    <li>Compare\n",
    "        <div style=\"display: flex\">\n",
    "            <table>\n",
    "                <tr><th>Percept</th><th>Action</th></tr>\n",
    "                <tr><td>00</td><td>MOVE</td></tr>\n",
    "                <tr><td>01</td><td>TURN(RIGHT, 2)</td></tr>\n",
    "                <tr><td>10</td><td>MOVE</td></tr>\n",
    "                <tr><td>11</td><td>TURN(LEFT, 2)</td></tr>\n",
    "            </table>\n",
    "            <table>\n",
    "                <tr><th>Percept</th><th>Action</th><th>$Q$</th></tr>\n",
    "                <tr><td>00</td><td>MOVE</td><td>&hellip;</td></tr>\n",
    "                <tr><td>00</td><td>TURN(RIGHT, 2)</td><td>&hellip;</td></tr>\n",
    "                <tr><td>00</td><td>TURN(LEFT, 2)</td><td>&hellip;</td></tr>\n",
    "                <tr><td>01</td><td>MOVE</td><td>&hellip;</td></tr>\n",
    "                <tr><td>01</td><td>TURN(RIGHT, 2)</td><td>&hellip;</td></tr>\n",
    "                <tr><td>01</td><td>TURN(LEFT, 2)</td><td>&hellip;</td></tr>\n",
    "                <tr><td>10</td><td>MOVE</td><td>&hellip;</td></tr>\n",
    "                <tr><td>10</td><td>TURN(RIGHT, 2)</td><td>&hellip;</td></tr>\n",
    "                <tr><td>10</td><td>TURN(LEFT, 2)</td><td>&hellip;</td></tr>\n",
    "                <tr><td>11</td><td>MOVE</td><td>&hellip;</td></tr>\n",
    "                <tr><td>11</td><td>TURN(RIGHT, 2)</td><td>&hellip;</td></tr>\n",
    "                <tr><td>11</td><td>TURN(LEFT, 2)</td><td>&hellip;</td></tr>\n",
    "            </table>\n",
    "        </div>\n",
    "    </li>\n",
    "    <li>Class exercise: Suppose the agent has $m$ touch sensors (returning 0 or 1) and $n$ different actions. \n",
    "        How many rows will the table contain?\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>What is $Q$?</h1>\n",
    "<ul>\n",
    "    <li>$Q(\\v{s}, a)$ is an <em>estimate</em> of the cumulative reward the agent will receive if, \n",
    "        having sensed $\\v{s}$, it chooses to execute action $a$\n",
    "    </li>\n",
    "    <li>Hence, having sensed $\\v{s}$, choose action $a$ for which $Q(\\v{s}, a)$ is highest:\n",
    "        $$\\argmax_a Q(\\v{s},a)$$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Class exercise</h1>\n",
    "<ul>\n",
    "    <li>Given this table\n",
    "        <table>\n",
    "            <tr><th>Percept</th><th>Action</th><th>$Q$</th></tr>\n",
    "            <tr><td>$\\vdots$</td><td>$\\vdots$</td><td>$\\vdots$</td></tr>\n",
    "            <tr><td>01</td><td>MOVE</td><td>0.2</td></tr>\n",
    "            <tr><td>01</td><td>TURN(RIGHT, 2)</td><td>0.1</td></tr>\n",
    "            <tr><td>01</td><td>TURN(LEFT, 2)</td><td>0.7</td></tr>\n",
    "            <tr><td>$\\vdots$</td><td>$\\vdots$</td><td>$\\vdots$</td></tr>\n",
    "        </table>   \n",
    "    </li>\n",
    "    <li>Suppose $\\v{s}$ is 01</li>\n",
    "    <li>What is $Q(\\v{s}, a)$?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>$Q$-learning</h1>\n",
    "<ul>\n",
    "    <li>Start wih random $Q$-values (or all zero)</li>\n",
    "    <li>Improve by trial-and-error: choose actions, get rewards, update $Q$-values</li>\n",
    "</ul>\n",
    "<figure style=\"border: 1px solid black; background-color: #D0D0D0\">\n",
    "    <figcaption style=\"border-bottom: 1px solid black\">\n",
    "        QLearning($\\epsilon$)\n",
    "    </figcaption>\n",
    "    <ul>\n",
    "        <li>$\\v{s} = \\mathit{SENSE}()$;</li>\n",
    "        <li>do forever\n",
    "            <ul>\n",
    "                <li>$\\mathit{rand} = $ a randomly-generated number in $[0,1)$;</li>\n",
    "                <li>if $\\mathit{rand} < \\epsilon$\n",
    "                    <ul>\n",
    "                        <li>Choose action $a$ randomly;</li>\n",
    "                    </ul>\n",
    "                    else\n",
    "                    <ul>\n",
    "                        <li>$a = \\argmax_a (\\v{s}, a)$;</li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li>$r = \\mathit{EXECUTE}(a)$;</li>\n",
    "                <li>$\\v{s}' = \\mathit{SENSE}()$;</li>\n",
    "                <li>$Q(\\v{s}, a) = r + \\gamma \\times \\max_{a'} Q(\\v{s}', a')$;</li>\n",
    "                <li>$\\v{s} = \\v{s}'$;</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Exploration vs. Exploitation</h1>\n",
    "<ul>\n",
    "    <li>Exploration:\n",
    "        <ul>\n",
    "            <li>Choose an action which may not be the best action according to the current $Q$ values.\n",
    "                But it may gain you new experience and improve the $Q$ values\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Exploitation:\n",
    "        <ul>\n",
    "            <li>Choose the action which is best according to the current $Q$ values.\n",
    "                It may gain you reward\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The so-called <b>$\\epsilon$-greedy policy</b> (where $0 \\leq \\epsilon \\leq 1$) is the simplest way to choose</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Updating $Q$-values</h1>\n",
    "<ul>\n",
    "    <li>From the algorithm:\n",
    "        $$Q(\\v{s}, a) = r + \\gamma \\times \\max_{a'}Q(\\v{s}', a')$$\n",
    "    </li>\n",
    "    <li>The new value is the reward for the latest action $r$ plus our highest current estimate of the cumulative reward\n",
    "        it can receive\n",
    "    </li>\n",
    "    <li>Over the course of repeated actions, the $Q$-values will get better and better:\n",
    "        <ul>\n",
    "            <li>When one $Q$-value improves then the $Q$-values of its immediate predecessors will also improve\n",
    "                next time they get updated\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Class exercise</h1>\n",
    "<ul>\n",
    "    <li>Given this table\n",
    "        <table>\n",
    "            <tr><th>Percept</th><th>Action</th><th>$Q$</th></tr>\n",
    "            <tr><td>$\\vdots$</td><td>$\\vdots$</td><td>$\\vdots$</td></tr>\n",
    "            <tr><td>10</td><td>MOVE</td><td>5</td></tr>\n",
    "            <tr><td>10</td><td>TURN(RIGHT, 2)</td><td>4</td></tr>\n",
    "            <tr><td>10</td><td>TURN(LEFT, 2)</td><td>1</td></tr>\n",
    "            <tr><td>11</td><td>MOVE</td><td>0</td></tr>\n",
    "            <tr><td>11</td><td>TURN(RIGHT, 2)</td><td>4</td></tr>\n",
    "            <tr><td>11</td><td>TURN(LEFT, 2)</td><td>6</td></tr>\n",
    "        </table>   \n",
    "    </li>\n",
    "    <li>Suppose current percept is 10<br />\n",
    "        Assuming exploitation, which action will be chosen?</li>\n",
    "    <li>Suppose reward is 3, next percept is 11 and $\\gamma$ is 1<br />\n",
    "        Using $Q(\\v{s}, a) = r + \\gamma \\times \\max_{a'} Q(\\v{s}', a')$, update the table\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Concluding remarks</h1>\n",
    "<ul>\n",
    "    <li>Reinforcement Learning underpins a lot of current success in game playing</li>\n",
    "    <li>But it is seeing real use in other areas, e.g. robot motion control</li>\n",
    "    <li>For real use, you need more sophisticated algorithms:\n",
    "        <ul>\n",
    "            <li>To handle non-deterministic environments</li>\n",
    "            <li>To improve convergence</li>\n",
    "            <li>To build a model of the environment</li>\n",
    "            <li>To scale up, and</li>\n",
    "            <li>To represent the policy in a way that allows the agent to generalise from what it learns</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>CS4619 may return to the last of these</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
