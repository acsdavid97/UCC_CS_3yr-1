{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Clustering: Introduction</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Class, for use in pipelines, to select certain columns from a DataFrame and convert to a numpy array\n",
    "# From A. Geron: Hands-On Machine Learning with Scikit-Learn & TensorFlow, O'Reilly, 2017\n",
    "# Modified by Derek Bridge to allow for casting in the same ways as pandas.DataFrame.astype\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype=None):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_selected = X[self.attribute_names]\n",
    "        if self.dtype:\n",
    "            return X_selected.astype(self.dtype).values\n",
    "        return X_selected.values\n",
    "    \n",
    "# Class, for use in pipelines, to binarize nominal-valued features (while avoiding the dummy variabe trap)\n",
    "# By Derek Bridge, 2017\n",
    "class FeatureBinarizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_values):\n",
    "        self.features_values = features_values\n",
    "        self.num_features = len(features_values)\n",
    "        self.labelencodings = [LabelEncoder().fit(feature_values) for feature_values in features_values]\n",
    "        self.onehotencoder = OneHotEncoder(sparse=False,\n",
    "            n_values=[len(feature_values) for feature_values in features_values])\n",
    "        self.last_indexes = np.cumsum([len(feature_values) - 1 for feature_values in self.features_values])\n",
    "    def fit(self, X, y=None):\n",
    "        for i in range(0, self.num_features):\n",
    "            X[:, i] = self.labelencodings[i].transform(X[:, i])\n",
    "        return self.onehotencoder.fit(X)\n",
    "    def transform(self, X, y=None):\n",
    "        for i in range(0, self.num_features):\n",
    "            X[:, i] = self.labelencodings[i].transform(X[:, i])\n",
    "        onehotencoded = self.onehotencoder.transform(X)\n",
    "        return np.delete(onehotencoded, self.last_indexes, axis=1)\n",
    "    def fit_transform(self, X, y=None):\n",
    "        onehotencoded = self.fit(X).transform(X)\n",
    "        return np.delete(onehotencoded, self.last_indexes, axis=1)\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"features_values\" : self.features_values}\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            self.setattr(parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Clustering</h1>\n",
    "<ul>\n",
    "    <li><b>Clustering</b> is the process of grouping objects according to some distance measure</li>\n",
    "    <li>The goals:\n",
    "        <ul>\n",
    "            <li>two objects in the same cluster are a small distance from each other</li>\n",
    "            <li>two objects in different clusters are a large distance from each other</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>E.g. how would you cluster these dogs? \n",
    "        <img style=\"float: right\" src=\"images/13_dogs.jpg\" />\n",
    "    </li>\n",
    "    <li>Applications:\n",
    "        <ul>\n",
    "            <li>Genetics: discovering groups of genes that express themselves in similar ways</li>\n",
    "            <li>Marketing: segmenting customers for targeted advertising or to drive new product development</li>\n",
    "            <li>Social network analysis: discovering communities in social networks</li>\n",
    "            <li>Social sciences: analysing populations based on demographics, behaviour, etc</li>\n",
    "            <li>Genetic algorithms: identifying population niches in an effort to maintain diversity</li>\n",
    "            <li>&hellip;</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Note: Clustering algorithms assign the objects to groups, but they are typically not capable\n",
    "        of giving meaningful labels (names) to the groups\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Clustering algorithms</h1>\n",
    "<ul>\n",
    "    <li>There are many, many algorithms, falling roughly into two kinds:\n",
    "        <ul>\n",
    "            <li><b>Point-assignment algorithms</b>: \n",
    "                <ul>\n",
    "                    <li>objects are initially assigned to clusters, e.g., arbitrarily</li>\n",
    "                    <li>then, repeatedly, each object is re-considered: it may be assigned to a cluster to \n",
    "                        which it is more closely related\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li><b>Hierarchical algorithms</b>: produce a tree of clusters\n",
    "                <ul>\n",
    "                    <li><b>Agglomerative algorithms</b> ('bottom-up'):\n",
    "                        <ul>\n",
    "                            <li>each object starts in a 'cluster' on its own; </li>\n",
    "                            <li>then, recursively, pairs of clusters are merged to form a parent cluster</li>\n",
    "                        </ul>\n",
    "                    </li>\n",
    "                    <li><b>Divisive algorithms</b> ('top-down'):\n",
    "                        <ul>\n",
    "                            <li>all objects start in a single cluster;</li>\n",
    "                            <li>then, recursively, a cluster is split into child clusters</li>\n",
    "                        </ul>\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>There are lots of other ways of distinguishing clustering algorithms from each other, e.g.\n",
    "        <ul>\n",
    "            <li>partitioning: must every object belong to exactly one cluster, or may some objects belong to more\n",
    "                than one cluster and may some objects belong to no cluster?\n",
    "            </li>\n",
    "            <li>hard vs. soft: is membership of a cluster Boolean (an object belongs to a cluster or it does not) \n",
    "                or is it fuzzy (there are degrees of membership, e.g. it is 0.8 true that this object belongs \n",
    "                to this cluster) or probabilistic\n",
    "            </li>\n",
    "            <li>whether they only work for certain distance measures (e.g. Euclidean, Manhattan, Chebyshev) and not\n",
    "                for others (e.g. cosine)\n",
    "            </li>\n",
    "            <li>whether they assume a dataset that fits into main memory or whether they scale to larger\n",
    "                datasets\n",
    "            </li>\n",
    "            <li>whether they assume all the data is available up-front, or whether they assume it arrives\n",
    "                over time\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We'll study two of the simpler algorithms: one point-assignment and one hierarchical</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>$k$-Means Clustering</h1>\n",
    "<ul>\n",
    "    <li>The <b>$k$-means algorithm</b> is the best-known <em>point-assignment algorithm</em>\n",
    "        <ul>\n",
    "            <li>E.g. the <code>KMeans</code> class in scikit-learn</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>It assumes that you know the number of clusters, $k$, in advance</li>\n",
    "    <li>Given a dataset of examples (as vectors) $\\v{X}$ it returns a <em>partition</em> of $\\v{X}$ into\n",
    "        $k$ subsets\n",
    "    </li>\n",
    "    <li>Key concept: the <b>centroid</b> of a cluster\n",
    "        <ul>\n",
    "            <li>the <em>mean</em> of the examples in that cluster, i.e. the mean of each feature\n",
    "            </li>\n",
    "        </ul>\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Centroids</h1>\n",
    "<ul>\n",
    "    <li>Class exercise: What are the centroids of these clusters?\n",
    "        <ol>\n",
    "            <li>$\\Set{\\cv{1\\\\1\\\\1}, \\cv{2\\\\4\\\\6}, \\cv{3\\\\7\\\\11}}$\n",
    "            <li>$\\Set{\\cv{4\\\\3\\\\7}}$</li>\n",
    "            <li>$\\Set{\\cv{2\\\\3}, \\cv{4\\\\2}}$</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>Observations:\n",
    "        <ul>\n",
    "            <li>The centroid of a cluster that contains just one example is the example itself</li>\n",
    "            <li>The centroid of a cluster that contains more than one example may not even be one of the\n",
    "                examples in the cluster\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>$k$-Means Algorithm</h1>\n",
    "<ul>\n",
    "    <li>It starts by choosing $k$ examples from $\\v{X}$ to be the initial centroids, e.g. randomly</li>\n",
    "    <li>Then, repeatedly,\n",
    "        <ul>\n",
    "            <li><b>Assignment step</b>: Each example $\\v{x} \\in \\v{X}$ is assigned to one of the clusters: \n",
    "                the one whose centroid is closest to $\\v{x}$\n",
    "            </li>\n",
    "            <li><b>Update step</b>: It re-computes the centroids of the clusters</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Toy Example, $k = 2$</h1>\n",
    "<table>\n",
    "    <tr style=\"border-bottom: 0\">\n",
    "        <td style=\"border-bottom: 0\">Dataset $\\v{X}$</td><td style=\"border-bottom: 0\">Random centroids</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-top: 0\">\n",
    "        <td style=\"border-top: 0\"><img src=\"images/13_km1.png\" /></td>\n",
    "        <td style=\"border-top: 0\"><img src=\"images/13_km2.png\" /></td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 0\">\n",
    "        <td style=\"border-bottom: 0\">Assignment step</td><td style=\"border-bottom: 0\">Update step</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-top: 0\">\n",
    "        <td style=\"border-top: 0\"><img src=\"images/13_km3.png\" /></td>\n",
    "        <td style=\"border-top: 0\"><img src=\"images/13_km4.png\" /></td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 0\">\n",
    "        <td style=\"border-bottom: 0\">Assignment step</td><td style=\"border-bottom: 0\">Update step</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-top: 0\">\n",
    "        <td style=\"border-top: 0\"><img src=\"images/13_km5.png\" /></td>\n",
    "        <td style=\"border-top: 0\"><img src=\"images/13_km6.png\" /></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>When to stop?</h1>\n",
    "<ul>\n",
    "    <li>If you run it for enough iterations, there <em>usually</em> comes a point when\n",
    "        <ul>\n",
    "            <li>In the update step, the centroids don't change</li>\n",
    "            <li>Hence, in the assignment step, the clustering doesn't change</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But there is a small risk that it <em>never</em> happens and that the algorithm oscillates between two or more\n",
    "        equaly good solutions\n",
    "    </li>\n",
    "    <li>Therefore, most implementations have a maximum number of iterations (<code>max_iter</code>\n",
    "        in scikit-learn)\n",
    "    </li>\n",
    "    <li>They might stop earlier, when the algorithm converges &mdash; next slide\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Inertia and Convergence</h1>\n",
    "<ul>\n",
    "    <li>What is $k$-means trying to achieve?\n",
    "        <ul>\n",
    "            <li>A clustering that <b>minimizes inertia</b>: the within-cluster sum of distances</li>\n",
    "            <li>I.e. the sum of the distances from each $\\v{x} \\in \\v{X}$ to its centroid is as low as possible\n",
    "            </li>\n",
    "        </ul>\n",
    "        (Advanced: The algorithm is more correctly formalized as trying to minimise the within-cluster\n",
    "        sum of squares of disstances, but with Euclidean distance, the best clustering is the same)\n",
    "    </li>\n",
    "    <li>If you run it for enough iterations, it will <b>converge</b>\n",
    "        <ul>\n",
    "            <li>I.e. the inertia will remain unchanged between iterations</li>\n",
    "        </ul>\n",
    "        The algorithm can stop at this point\n",
    "    </li>\n",
    "    <li>Most implementations have a tolerance (<code>tol</code> in scikit-learn):\n",
    "        <ul>\n",
    "            <li>They stop when the change in inertia falls below the tolerance, rather than waiting\n",
    "                for zero change\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Local and Global Minima</h1>\n",
    "<ul>\n",
    "    <li>Even if the algorithm converges (no improvement in inertia), the clustering it converges on\n",
    "        might not be the <b>global minimum</b> (the one with lowest possible inertia)</li>\n",
    "    <li>$k$-means produces different clustering depending on the choice of the initial $k$ centroids</li>\n",
    "    <li>For a given set of initial centroids, the clustering it converges on might be a <b>local minimum</b>:\n",
    "        <ul>\n",
    "            <li>For these initial centroids, no better clustering can be found, but it's not\n",
    "                the very best clustering possible\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Class exercise. Here, $\\v{X}$ contains four examples at the corners of a rectangle:\n",
    "        <figure>\n",
    "            <img src=\"images/13_minima.png\" />\n",
    "        </figure>\n",
    "        <ul>\n",
    "            <li>For $k=2$, choose initial centroids that result in a global minimum</li>\n",
    "            <li>And choose $k=2$ centroids that give a local minimum</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Let's look at ways of reducing the problem&hellip;</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Avoiding local minima: re-running</h2>\n",
    "<ul>\n",
    "    <li>The obvious solution is to run $k$-means multiple times (with different initial centroids) and\n",
    "        return the clustering that has the lowest inertia\n",
    "    </li>\n",
    "    <li>No guarantee of finding the global minimum this way but likely to be better</li>\n",
    "    <li>E.g. scikit-learn the number of runs (<code>n_init</code>) is 10, by default</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Avoiding local minima: better initial centroids</h2>\n",
    "<ul>\n",
    "    <li>Choosing the initial $k$ centroids at random from $\\v{X}$ has problems:\n",
    "        <ul>\n",
    "            <li>The algorithm can return different clusters for $\\v{X}$ each time it is run\n",
    "            </li>\n",
    "            <li>The clustering it returns may be a local minima</li>\n",
    "            <li>A poor choice can increase the number of iterations needed for convergence\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>There are many alternatives to choosing wholly randomly, e.g.:\n",
    "        <ul>\n",
    "            <li>insert into $\\mathit{Centroids}$ an example $\\v{x} \\in \\v{X}$,\n",
    "                chosen at random with uniform probability\n",
    "            </li>\n",
    "            <li>while $|\\mathit{Centroids}| < k$\n",
    "                <ul>\n",
    "                    <li>insert into $\\mathit{Centroids}$ a different example $\\v{x} \\in \\v{X}$,\n",
    "                        chosen with probability proportional to $(\\min_{\\v{x}' \\in \\mathit{Centroids}}\\dist(\\v{x}, \\v{x}'))^2$\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "     </li>\n",
    "     <li>$k$-means++ is the name of the $k$-means algorithm when using the above method\n",
    "         <ul>\n",
    "             <li>it still has randomness, so it still suffers from the problems above, but typically less so</li>\n",
    "         </ul>\n",
    "     </li>\n",
    "     <li>In scikit-learn, the <code>init</code> parameter can have values <code>‘k-means++’</code> (default)\n",
    "         or <code>‘random'</code>\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>$k$-means clustering: discussion</h1>\n",
    "<ul>\n",
    "    <li>$k$-means can work well\n",
    "        <ul>\n",
    "            <li>but not so much in the presence of outliers or when the natural clusters are elongated or \n",
    "                irregular shapes\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The curse of dimensionality may be relevant\n",
    "        <ul>\n",
    "            <li>You might want to include dimensionality reduction such as PCA in your pipeline</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The algorithm mostly scales well to larger data\n",
    "        <ul>\n",
    "            <li>There are variants for speed-up, e.g. <code>MiniBatchKMeans</code> in scikit-learn\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>There is the problem of choosing $k$ in advance\n",
    "        <ul>\n",
    "            <li>Why does it not make sense to run it with all $k$ in $[2,m]$ and choose the clustering\n",
    "                with lowest inertia?\n",
    "            </li>\n",
    "            <li>There are point-assignment algorithms that do not require you to choose $k$ in advance</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>$k$-Means in scikit-learn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>type</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>floors</th>\n",
       "      <th>devment</th>\n",
       "      <th>ber</th>\n",
       "      <th>location</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>207.000000</td>\n",
       "      <td>207</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Semi-detached</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SecondHand</td>\n",
       "      <td>G</td>\n",
       "      <td>CityCentre</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>204</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>128.094686</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.434783</td>\n",
       "      <td>2.106280</td>\n",
       "      <td>1.826087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>274.724638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>73.970582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.232390</td>\n",
       "      <td>1.185802</td>\n",
       "      <td>0.379954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>171.756507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>41.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>82.650000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>165.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>225.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>153.650000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>327.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>497.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>995.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            flarea           type       bdrms      bthrms      floors  \\\n",
       "count   207.000000            207  207.000000  207.000000  207.000000   \n",
       "unique         NaN              4         NaN         NaN         NaN   \n",
       "top            NaN  Semi-detached         NaN         NaN         NaN   \n",
       "freq           NaN             65         NaN         NaN         NaN   \n",
       "mean    128.094686            NaN    3.434783    2.106280    1.826087   \n",
       "std      73.970582            NaN    1.232390    1.185802    0.379954   \n",
       "min      41.800000            NaN    1.000000    1.000000    1.000000   \n",
       "25%      82.650000            NaN    3.000000    1.000000    2.000000   \n",
       "50%     106.000000            NaN    3.000000    2.000000    2.000000   \n",
       "75%     153.650000            NaN    4.000000    3.000000    2.000000   \n",
       "max     497.000000            NaN   10.000000   10.000000    2.000000   \n",
       "\n",
       "           devment  ber    location       price  \n",
       "count          207  207         207  207.000000  \n",
       "unique           2   12          36         NaN  \n",
       "top     SecondHand    G  CityCentre         NaN  \n",
       "freq           204   25          40         NaN  \n",
       "mean           NaN  NaN         NaN  274.724638  \n",
       "std            NaN  NaN         NaN  171.756507  \n",
       "min            NaN  NaN         NaN   55.000000  \n",
       "25%            NaN  NaN         NaN  165.000000  \n",
       "50%            NaN  NaN         NaN  225.000000  \n",
       "75%            NaN  NaN         NaN  327.500000  \n",
       "max            NaN  NaN         NaN  995.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/dataset_corkA.csv\")\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_features = [\"flarea\", \"bdrms\", \"bthrms\", \"floors\"]\n",
    "nominal_features = [\"type\", \"devment\", \"ber\", \"location\"]\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"selector\", DataFrameSelector(numeric_features)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "nominal_pipeline = Pipeline([\n",
    "    (\"selector\", DataFrameSelector(nominal_features)),\n",
    "    (\"binarizer\", FeatureBinarizer([df[feature].unique() for feature in nominal_features]))\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"union\", FeatureUnion([\n",
    "        (\"numeric_feature\", numeric_pipeline),\n",
    "        (\"nominal_feature\", nominal_pipeline),\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(df)\n",
    "X = pipeline.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 2\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on KMeans in module sklearn.cluster.k_means_ object:\n",
      "\n",
      "class KMeans(sklearn.base.BaseEstimator, sklearn.base.ClusterMixin, sklearn.base.TransformerMixin)\n",
      " |  K-Means clustering\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <k_means>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |  n_clusters : int, optional, default: 8\n",
      " |      The number of clusters to form as well as the number of\n",
      " |      centroids to generate.\n",
      " |  \n",
      " |  init : {'k-means++', 'random' or an ndarray}\n",
      " |      Method for initialization, defaults to 'k-means++':\n",
      " |  \n",
      " |      'k-means++' : selects initial cluster centers for k-mean\n",
      " |      clustering in a smart way to speed up convergence. See section\n",
      " |      Notes in k_init for more details.\n",
      " |  \n",
      " |      'random': choose k observations (rows) at random from data for\n",
      " |      the initial centroids.\n",
      " |  \n",
      " |      If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
      " |      and gives the initial centers.\n",
      " |  \n",
      " |  n_init : int, default: 10\n",
      " |      Number of time the k-means algorithm will be run with different\n",
      " |      centroid seeds. The final results will be the best output of\n",
      " |      n_init consecutive runs in terms of inertia.\n",
      " |  \n",
      " |  max_iter : int, default: 300\n",
      " |      Maximum number of iterations of the k-means algorithm for a\n",
      " |      single run.\n",
      " |  \n",
      " |  tol : float, default: 1e-4\n",
      " |      Relative tolerance with regards to inertia to declare convergence\n",
      " |  \n",
      " |  precompute_distances : {'auto', True, False}\n",
      " |      Precompute distances (faster but takes more memory).\n",
      " |  \n",
      " |      'auto' : do not precompute distances if n_samples * n_clusters > 12\n",
      " |      million. This corresponds to about 100MB overhead per job using\n",
      " |      double precision.\n",
      " |  \n",
      " |      True : always precompute distances\n",
      " |  \n",
      " |      False : never precompute distances\n",
      " |  \n",
      " |  verbose : int, default 0\n",
      " |      Verbosity mode.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default: None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  copy_x : boolean, default True\n",
      " |      When pre-computing distances it is more numerically accurate to center\n",
      " |      the data first.  If copy_x is True, then the original data is not\n",
      " |      modified.  If False, the original data is modified, and put back before\n",
      " |      the function returns, but small numerical differences may be introduced\n",
      " |      by subtracting and then adding the data mean.\n",
      " |  \n",
      " |  n_jobs : int\n",
      " |      The number of jobs to use for the computation. This works by computing\n",
      " |      each of the n_init runs in parallel.\n",
      " |  \n",
      " |      If -1 all CPUs are used. If 1 is given, no parallel computing code is\n",
      " |      used at all, which is useful for debugging. For n_jobs below -1,\n",
      " |      (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one\n",
      " |      are used.\n",
      " |  \n",
      " |  algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\"\n",
      " |      K-means algorithm to use. The classical EM-style algorithm is \"full\".\n",
      " |      The \"elkan\" variation is more efficient by using the triangle\n",
      " |      inequality, but currently doesn't support sparse data. \"auto\" chooses\n",
      " |      \"elkan\" for dense data and \"full\" for sparse data.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cluster_centers_ : array, [n_clusters, n_features]\n",
      " |      Coordinates of cluster centers\n",
      " |  \n",
      " |  labels_ :\n",
      " |      Labels of each point\n",
      " |  \n",
      " |  inertia_ : float\n",
      " |      Sum of distances of samples to their closest cluster center.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  \n",
      " |  >>> from sklearn.cluster import KMeans\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      " |  ...               [4, 2], [4, 4], [4, 0]])\n",
      " |  >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
      " |  >>> kmeans.labels_\n",
      " |  array([0, 0, 0, 1, 1, 1], dtype=int32)\n",
      " |  >>> kmeans.predict([[0, 0], [4, 4]])\n",
      " |  array([0, 1], dtype=int32)\n",
      " |  >>> kmeans.cluster_centers_\n",
      " |  array([[ 1.,  2.],\n",
      " |         [ 4.,  2.]])\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  \n",
      " |  MiniBatchKMeans\n",
      " |      Alternative online implementation that does incremental updates\n",
      " |      of the centers positions using mini-batches.\n",
      " |      For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n",
      " |      probably much faster than the default batch implementation.\n",
      " |  \n",
      " |  Notes\n",
      " |  ------\n",
      " |  The k-means problem is solved using Lloyd's algorithm.\n",
      " |  \n",
      " |  The average complexity is given by O(k n T), were n is the number of\n",
      " |  samples and T is the number of iteration.\n",
      " |  \n",
      " |  The worst case complexity is given by O(n^(k+2/p)) with\n",
      " |  n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n",
      " |  'How slow is the k-means method?' SoCG2006)\n",
      " |  \n",
      " |  In practice, the k-means algorithm is very fast (one of the fastest\n",
      " |  clustering algorithms available), but it falls in local minima. That's why\n",
      " |  it can be useful to restart it several times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KMeans\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClusterMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute k-means clustering.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |          Training instances to cluster.\n",
      " |  \n",
      " |  fit_predict(self, X, y=None)\n",
      " |      Compute cluster centers and predict cluster index for each sample.\n",
      " |      \n",
      " |      Convenience method; equivalent to calling fit(X) followed by\n",
      " |      predict(X).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      labels : array, shape [n_samples,]\n",
      " |          Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Compute clustering and transform X to cluster-distance space.\n",
      " |      \n",
      " |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array, shape [n_samples, k]\n",
      " |          X transformed in the new space.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the closest cluster each sample in X belongs to.\n",
      " |      \n",
      " |      In the vector quantization literature, `cluster_centers_` is called\n",
      " |      the code book and each value returned by `predict` is the index of\n",
      " |      the closest code in the code book.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data to predict.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      labels : array, shape [n_samples,]\n",
      " |          Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Opposite of the value of X on the K-means objective.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Opposite of the value of X on the K-means objective.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X to a cluster-distance space.\n",
      " |      \n",
      " |      In the new space, each dimension is the distance to the cluster\n",
      " |      centers.  Note that even if X is sparse, the array returned by\n",
      " |      `transform` will typically be dense.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array, shape [n_samples, k]\n",
      " |          X transformed in the new space.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The features we want to select\n",
    "numeric_features = [\"flarea\", \"bdrms\", \"bthrms\", \"floors\"]\n",
    "nominal_features = [\"type\", \"devment\", \"ber\", \"location\"]\n",
    "\n",
    "# Create the pipelines\n",
    "numeric_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector(numeric_features)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "nominal_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector(nominal_features)), \n",
    "        (\"binarizer\", FeatureBinarizer([df[feature].unique() for feature in nominal_features]))])\n",
    "\n",
    "pipeline = Pipeline([(\"union\", FeatureUnion([(\"numeric_pipeline\", numeric_pipeline), \n",
    "                                             (\"nominal_pipeline\", nominal_pipeline)]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "pipeline.fit(df)\n",
    "X = pipeline.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the clustering object\n",
    "\n",
    "k = 2\n",
    "kmeans = KMeans(n_clusters=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run it\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983.77017887121383"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In case you're interested, you can see the final inertia\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.31820283e-01,  -3.64013409e-01,  -3.48297304e-01,\n",
       "         -1.29320334e-01,   1.65605096e-01,   1.46496815e-01,\n",
       "          3.75796178e-01,   9.80891720e-01,   6.36942675e-03,\n",
       "          1.91082803e-02,   1.01910828e-01,   7.64331210e-02,\n",
       "          7.00636943e-02,   9.55414013e-02,   1.01910828e-01,\n",
       "          1.27388535e-01,   8.28025478e-02,   8.28025478e-02,\n",
       "          1.33757962e-01,   3.18471338e-02,   1.91082803e-02,\n",
       "          1.91082803e-02,   2.54777070e-02,  -1.30104261e-17,\n",
       "          1.91082803e-02,   3.18471338e-02,   1.01910828e-01,\n",
       "          6.36942675e-03,   2.42038217e-01,  -1.30104261e-17,\n",
       "          1.27388535e-02,   1.08280255e-01,  -1.30104261e-17,\n",
       "          2.54777070e-02,   6.36942675e-03,   7.64331210e-02,\n",
       "          1.91082803e-02,   2.54777070e-02,   6.36942675e-03,\n",
       "         -2.60208521e-17,   1.91082803e-02,   6.36942675e-03,\n",
       "         -2.42861287e-17,   1.91082803e-02,   2.54777070e-02,\n",
       "          5.09554140e-02,   1.27388535e-02,   6.36942675e-03,\n",
       "          1.27388535e-02,   2.54777070e-02,   1.27388535e-02,\n",
       "         -5.20417043e-17,   6.36942675e-03,   1.91082803e-02],\n",
       "       [  1.35591569e+00,   1.14300210e+00,   1.09365353e+00,\n",
       "          4.06065849e-01,   2.00000000e-02,   7.60000000e-01,\n",
       "          1.20000000e-01,   1.00000000e+00,   4.33680869e-18,\n",
       "          1.40000000e-01,   8.00000000e-02,   8.00000000e-02,\n",
       "          1.20000000e-01,   1.60000000e-01,   1.00000000e-01,\n",
       "          6.00000000e-02,   1.00000000e-01,   6.00000000e-02,\n",
       "          8.00000000e-02,  -1.73472348e-17,   1.21430643e-17,\n",
       "          1.21430643e-17,   4.00000000e-02,   2.00000000e-02,\n",
       "          4.00000000e-02,   2.00000000e-02,   1.40000000e-01,\n",
       "          2.00000000e-02,   4.00000000e-02,   2.00000000e-02,\n",
       "          8.67361738e-18,   2.20000000e-01,   2.00000000e-02,\n",
       "          1.73472348e-17,   4.33680869e-18,   2.00000000e-02,\n",
       "          1.21430643e-17,   1.73472348e-17,   4.33680869e-18,\n",
       "          4.00000000e-02,   1.21430643e-17,   2.00000000e-02,\n",
       "          6.00000000e-02,   1.21430643e-17,   8.00000000e-02,\n",
       "          8.00000000e-02,   8.67361738e-18,   2.00000000e-02,\n",
       "          8.67361738e-18,   1.73472348e-17,   8.67361738e-18,\n",
       "          8.00000000e-02,   4.33680869e-18,   2.00000000e-02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...and even the vectors of the final centroids\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The clusters have been labeled (numbered) from 0...(k-1)\n",
    "# We can see the labels of each example in the dataset\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's hack up a function that helps us look at a few examples from each cluster\n",
    "def inspect_clusters(alg, df, k, features_to_show, how_many_to_show=None):\n",
    "    for i in range(0, k):\n",
    "        print(\"A few examples from cluster \", i)\n",
    "        indexes = alg.labels_ == i\n",
    "        max_available = indexes.sum()\n",
    "        print(df.ix[indexes, features_to_show]\n",
    "                   [:max_available if not how_many_to_show else min(how_many_to_show, max_available)])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few examples from cluster  0\n",
      "   flarea  bdrms  bthrms  floors           type     devment ber  location\n",
      "1    83.6      3       1       1       Detached  SecondHand  D2  Glanmire\n",
      "2    97.5      3       2       2  Semi-detached  SecondHand  D1  Glanmire\n",
      "4   118.7      3       2       2  Semi-detached  SecondHand  E2   Douglas\n",
      "\n",
      "A few examples from cluster  1\n",
      "   flarea  bdrms  bthrms  floors           type     devment ber      location\n",
      "0   497.0      4       5       2       Detached  SecondHand  B2  Carrigrohane\n",
      "3   158.0      5       2       2  Semi-detached  SecondHand  C3       Douglas\n",
      "5   170.0      4       4       2       Detached  SecondHand  D1       Douglas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show 3 examples from each cluster for KMeans with random initialization\n",
    "inspect_clusters(kmeans, df, k, numeric_features + nominal_features, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Go back and try with a different value for $k$</li>\n",
    "    <li>But eye-balling examples from the clusters is not a reliable way of judging the quality of the clustering</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Evaluating clustering, part 1</h1>\n",
    "<ul>\n",
    "    <li>Suppose someone has already done a manual clustering of the dataset ('ground truth'):\n",
    "        <ul>\n",
    "            <li>Then you can compare the output of the algorithm with the ground truth</li>\n",
    "            <li>Discussed in next lecture</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Suppose you don't have a ground truth (much more typical!):\n",
    "        <ul>\n",
    "            <li><b>Silhouette Coefficient</b> is one of several ways of scoring clustering quality:\n",
    "                <ul>\n",
    "                    <li>For each example $\\v{x} \\in \\v{X}$, compute \n",
    "                        $$\\frac{b - a}{max(a, b)}$$\n",
    "                        where $a$ is the mean distance between $\\v{x}$ and all other examples in the same cluster,\n",
    "                        $b$ is the mean distance between $\\v{x}$ and all examples in the next nearest cluster\n",
    "                    </li>\n",
    "                    <li>The Silhouette Coefficient is the mean of all of these</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                Its values lies in $[-1,1]$:\n",
    "                <ul>\n",
    "                    <li>Positive values suggest examples are in their correct clusters</li>\n",
    "                    <li>Values near 0 indicate clusters that are not well separated</li>\n",
    "                    <li>Negative values suggest examples are in the wrong clusters</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25888899875904836"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(X, kmeans.labels_, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
