{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Accuracy Estimation</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Class, for use in pipelines, to select certain columns from a DataFrame and convert to a numpy array\n",
    "# From A. Geron: Hands-On Machine Learning with Scikit-Learn & TensorFlow, O'Reilly, 2017\n",
    "# Modified by Derek Bridge to allow for casting in the same ways as pandas.DataFrame.astype\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype=None):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_selected = X[self.attribute_names]\n",
    "        if self.dtype:\n",
    "            return X_selected.astype(self.dtype).values\n",
    "        return X_selected.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>The CS1109 Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/dataset_cs1109.csv\")\n",
    "\n",
    "df = df.take(np.random.permutation(len(df)))\n",
    "\n",
    "y = df[\"outcome\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The features we want to select\n",
    "features = [\"lect\", \"lab\", \"cao\"]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector(features, \"float64\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"estimator\", LogisticRegression())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Evaluation Methods for Classification</h1>\n",
    "<ul>\n",
    "    <li>Holdout (and many other methods) rely on <em>randomly</em>\n",
    "        partitioning the dataset into a training set and a test set\n",
    "        <ul>\n",
    "            <li>We've discussed before that the split may be 'lucky' or 'unlucky', hence resampling methods\n",
    "                such as $k$-Fold Cross-Validation\n",
    "            </li>\n",
    "        </ul>\n",
    "    <li>In the case of classification, for example, the split might not reflect\n",
    "        the distribution of examples within the classes:\n",
    "        <ul>\n",
    "            <li>Examples of one class might be under-represented in the training set or test set</li>\n",
    "            <li>Examples of one class might even be completely absent from the training set or test set</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Stratification</b> is the solution to this: \n",
    "        <ul>\n",
    "            <li>In stratification, the proportion of examples of each class in the overall dataset is respected \n",
    "                in the partitioning into training and test sets\n",
    "            </li>\n",
    "            <li>Here's pseudocode for <b>stratified holdout</b>. For simplicity, the pseudocode only \n",
    "                covers the case of binary classification:\n",
    "                <ul style=\"background: lightgray\">\n",
    "                    <li>Divide the dataset into positive examples, $P$, and negative examples, $N$</li>\n",
    "                    <li>Randomly partition $P$ into $\\mathit{Train}_P$ and $\\mathit{Test}_P$</li>\n",
    "                    <li>Randomly partition $N$ into $\\mathit{Train}_N$ and $\\mathit{Test}_N$</li>\n",
    "                    <li>$\\mathit{Train} \\gets \\mathit{Train}_P \\cup \\mathit{Train}_N$</li>\n",
    "                    <li>$\\mathit{Test} \\gets \\mathit{Test}_P \\cup \\mathit{Test}_N$</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Although this fixes the distribution with respect to the classes, you may still get 'lucky' or 'unlucky' \n",
    "        in other ways, so you will still want to do the above multiple times, e.g. <b>Stratified \n",
    "        $k$-Fold Cross-Validation</b>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Stratification in scikit-learn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stratified holdout\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits=1, train_size=0.8)\n",
    "\n",
    "np.mean(cross_val_score(pipeline, df, y, scoring=\"accuracy\", cv=ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88571428571428568"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the following, which doesn't use stratification, to the above, which does\n",
    "\n",
    "ss = ShuffleSplit(n_splits=1, train_size=0.8)\n",
    "\n",
    "np.mean(cross_val_score(pipeline, df, y, scoring=\"accuracy\", cv=ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79567099567099575"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stratified k-fold cross-validation\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 10)\n",
    "np.mean(cross_val_score(pipeline, df, y, scoring=\"accuracy\", cv=kf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78689075630252092"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again compare when we don't use stratification\n",
    "\n",
    "kf = KFold(n_splits = 10)\n",
    "np.mean(cross_val_score(pipeline, df, y, scoring=\"accuracy\", cv=kf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79567099567099575"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you use the shorthand on a classifier, you'll get stratification \n",
    "\n",
    "np.mean(cross_val_score(pipeline, df, y, scoring=\"accuracy\", cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Performance Measurement for Classification</h1>\n",
    "<ul>\n",
    "    <li>We saw before that the performance measure we use to evaluate a predictor might be different\n",
    "        from the loss function we used when training it\n",
    "        <ul>\n",
    "            <li>E.g. for regression we might use MAE or RMSE to measure performance, but (half of) MSE as the loss function\n",
    "            </li>\n",
    "        </ul>\n",
    "        The same is true for classification but there is an even greater choice of performance measures\n",
    "    </li>  \n",
    "    <li>The obvious performance measure for a classifier is <b>accuracy</b> (used above): the ratio of the number of correct \n",
    "        predictions to number of predictions\n",
    "        made\n",
    "        <ul>\n",
    "            <li>If you like notation:\n",
    "                $$\\frac{1}{|T|}\\sum_{i = 1}^{|T|} I(\\hat{y}^{(i)} \\neq y^{(i)})$$\n",
    "                where $T$ is the set of examples on which you are testing the classifier and hence $|T|$ is \n",
    "                the number of examples, and $I(p)$ is the indicator function that outputs 1\n",
    "                if predicate $p$ is true and zero otherwise\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Confusion Matrix</h1>\n",
    "<ul>\n",
    "    <li>\n",
    "        The <b>confusion matrix</b> $CM$ for a classifier is a square $|C| \\times |C|$ matrix\n",
    "        <ul>\n",
    "            <li>In a confusion matrix, a cell $CM[i, j]$\n",
    "                contains the number of test examples of class $i$ that were classified as class $j$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        Here are examples of confusion matrices for a binary classifier and a multiclass classifier:\n",
    "        <div>\n",
    "            <table style=\"float: left\">\n",
    "                <tr>\n",
    "                    <th colspan=\"2\" rowspan=\"2\"></th><th colspan=\"2\">Predicted Class</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>0</th><th>1</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th rowspan=\"2\">Actual Class</th><th>0</th><td>25</td><td>10</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>1</th><td>20</td><td>45</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th colspan=\"2\" rowspan=\"2\"></th><th colspan=\"3\">Predicted Class</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>0</th><th>1</th><th>2</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th rowspan=\"3\">Actual Class</th><th>0</th><td>10</td><td>0</td><td>15</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>1</th><td>5</td><td>30</td><td>10</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>2</th><td>5</td><td>5</td><td>20</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </div>\n",
    "    </li>\n",
    "    <li>Let's assume a test set $T$\n",
    "        <ul>\n",
    "            <li>\n",
    "                The sum of all entries in $CM$ equals $|T|$\n",
    "            </li>\n",
    "            <li>\n",
    "                The sum of the entries in <em>row</em> $i$ is\n",
    "                the number of examples in $T$ that have class $i$\n",
    "            </li>\n",
    "            <li>\n",
    "                The sum of the entries in <em>column</em> $j$ is\n",
    "                the number of examples in $T$ that the classifier assigns to class $j$\n",
    "            </li>\n",
    "            <li>\n",
    "                Entries on the main diagonal $CM[i,i]$, are correctly classified, and so $\\sum_i CM[i,i]$ is\n",
    "                the total number of correctly classified examples.\n",
    "            </li>\n",
    "            <li>\n",
    "                Entries off the main diagonal, $CM[i, j], i\\neq j$, are incorrectly classified, and so \n",
    "                $\\sum_i\\sum_{j, j \\neq i} CM[i,j]$ is the total number of incorrectly classified examples.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>So, in a word, what does this calculate?\n",
    "        $$\\frac{\\sum_i CM[i,i]}{|T|}$$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Confusion Matrices for Binary Classification</h1>\n",
    "<ul>\n",
    "    <li>In the case of <em>binary classification</em>, where we distinguish a positive class from a negative class,\n",
    "    there is some special terminology associated with the cells of the confusion matrix:\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th colspan=\"2\" rowspan=\"2\"></th><th colspan=\"2\">Predicted Class</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th>0</th><th>1</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th rowspan=\"2\">Actual Class</th><th>0</th><td>True Negatives</td><td>False Positives</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th>1</th><td>False Negatives</td><td>True Positives</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </li>\n",
    "    <li>The True Negatives (TN) and True Positives (TP) are correct classifications</li>\n",
    "    <li>The False Negatives (FN) and False Positives (FP) are incorrect classifications</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Confusion Matrices in scikit-learn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[182,  24],\n",
       "       [ 46,  90]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = cross_val_predict(pipeline, df, y, cv=10) # NB cross-val_predict, not cross_val_score\n",
    "confusion_matrix(y, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Limitations of Classification Accuracy</h1>\n",
    "<ul>\n",
    "    <li>Accuracy summarizes overall performance in a single figure, which is a good thing</li>\n",
    "    <li>But it has at least three problems:   \n",
    "        <ol>\n",
    "            <li>\n",
    "                Giving only a single figure hides information about how the classifier performs on the individual \n",
    "                classes\n",
    "                <ul>\n",
    "                    <li>This problem becomes more acute when the costs of different kinds of misclassification \n",
    "                        are not equal\n",
    "                    </li>\n",
    "                    <li>For example, in email classification, it is more serious to misclassify ham as spam\n",
    "                    </li>\n",
    "                    <li>Class exercise: Classifier A and classifier B have the same classification accuracy \n",
    "                        (0.6) but which classifier would you use?\n",
    "                        <table style=\"float: left\">\n",
    "                            <tr>\n",
    "                                <th colspan=\"2\" rowspan=\"2\">Classifier A</th><th colspan=\"2\">Predicted Class</th>\n",
    "                            </tr>\n",
    "                            <tr>\n",
    "                                <th>Benign</th><th>Malignant</th>\n",
    "                            </tr>\n",
    "                            <tr>\n",
    "                                <th rowspan=\"2\">Actual Class</th><th>Benign</th><td>400</td><td>100</td>\n",
    "                            </tr>\n",
    "                            <tr>\n",
    "                                <th>Malignant</th><td>300</td><td>200</td>\n",
    "                            </tr>\n",
    "                        </table>\n",
    "                        <table>\n",
    "                            <tr>\n",
    "                                <th colspan=\"2\" rowspan=\"2\">Classifier B</th><th colspan=\"2\">Predicted Class</th>\n",
    "                            </tr>\n",
    "                            <tr>\n",
    "                                <th>Benign</th><th>Malignant</th>\n",
    "                            </tr>\n",
    "                            <tr>\n",
    "                                <th rowspan=\"2\">Actual Class</th><th>Benign</th><td>200</td><td>300</td>\n",
    "                            </tr>\n",
    "                            <tr>\n",
    "                                <th>Malignant</th><td>100</td><td>400</td>\n",
    "                            </tr>\n",
    "                        </table>\n",
    "                    </li>\n",
    "                    <li>In principle, we could assign costs to the different kinds of mis-classification and define\n",
    "                        a cost-sensitive variant of classification accuracy, but, in practice, it's difficult, if\n",
    "                        not impossible, to come up with the costs; for example, how much worse is it to \n",
    "                        classify ham as spam than spam as ham?\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                Classification accuracy is also best when the distribution of classes in $T$ is reasonably\n",
    "                balanced\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        If, on the other hand, some of the classes are more prevalent than others, then they \n",
    "                        tend to bias the measure, e.g. if you do well on the more prevalent classes, then you get \n",
    "                        a higher score overall\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Accuracy does not take into account correct classifications from mere chance\n",
    "                <ul>\n",
    "                    <li>There are performance measures that <em>correct for chance</em>\n",
    "                        (but we won't look at them!)\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ol>       \n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Majority-Class Classifier</h1>\n",
    "<ul>\n",
    "    <li>\n",
    "        Consider $T$ that contains 950 positive examples and 50 negative examples\n",
    "    </li>\n",
    "    <li>An extremely effective classifier in terms of accuracy for this $T$ is the so-called \n",
    "       <b>majority-class classifier</b>,\n",
    "       <ul>\n",
    "           <li>It <em>always</em> predicts the majority class</li>\n",
    "       </ul>\n",
    "    <li>In this example, it predicts the positive class, and its accuracy is very high: 0.95</li>\n",
    "    <li>But it isn't really a good classifier: it has no ability to discriminate between\n",
    "        positive and negative examples\n",
    "    </li>\n",
    "    <li>Many people compare the accuracy of their classifier(s) against the \n",
    "        accuracy of the majority-class classifier to check that they are doing better\n",
    "        than this simple-minded baseline\n",
    "        <ul>\n",
    "            <li>It is one way of partly overcoming the second and third problems on the previous slide</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the classifier (pipeline not needed)\n",
    "pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector(features)),    \n",
    "        (\"estimator\", DummyClassifier(strategy = \"most_frequent\"))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60242424242424253"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(pipeline, df, y, scoring=\"accuracy\", cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Exercise</h1>\n",
    "<ul>\n",
    "    <li>The majority-class classifier is a 'baseline' that we can comparae against when evaluating a classifier</li>\n",
    "    <li>Propose a baseline that you can compare against when evaluating a regressor</li>\n",
    "    <li>Is it in scikit-learn?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Other Performance Measures for Classification</h1>\n",
    "<ul>\n",
    "    <li>If you do any work on classification beyond this module, you'd need to make yourself aware of all the\n",
    "        other performance measures that you could use:\n",
    "        <ul>\n",
    "            <li>E.g. precision recall, roc curves, &hellip;\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Beware of 'fishing expeditions':\n",
    "        <ul>\n",
    "            <li>Choose a measure or two in advance of running any experiments &mdash; the measure(s) that you think\n",
    "                are best-aligned to your business problem\n",
    "            </li>\n",
    "            <li>When there are many measures, there is a temptation to calculate them all and then get excited when\n",
    "                the learner performs well on one of them\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Example: the Iris Dataset</h1>\n",
    "<ul>\n",
    "    <li>The famous Iris dataset: \n",
    "        <a href=\"https://archive.ics.uci.edu/ml/datasets/iris\">https://archive.ics.uci.edu/ml/datasets/iris</a>\n",
    "    </li>\n",
    "    <li>$m = 150$ examples: each one is a flower &mdash; in fact, an Iris</li>\n",
    "    <li>$n = 4$ features: sepal length, sepal width, petal length and petal width (all in centimetres)</li>\n",
    "    <li>Three classes (different kinds of Iris): <i>Iris setosa</i>, <i>Iris versicolor</i> and\n",
    "        <i>Iris virginica</i>\n",
    "        <img src=\"images/20_irises.png\" />\n",
    "    </li>\n",
    "    <li>Big warning: \n",
    "        <ul>\n",
    "            <li>The dataset is sorted: 50 setosa then 50 versicolor then 50 virginica</li>\n",
    "            <li>So shuffling is important</li>\n",
    "        </ul>\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/dataset_iris.csv\")\n",
    "\n",
    "df = df.take(np.random.permutation(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'label'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    float64\n",
       "sepal_width     float64\n",
       "petal_length    float64\n",
       "petal_width     float64\n",
       "label            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sepal_length  sepal_width  petal_length  petal_width           label\n",
       "count     150.000000   150.000000    150.000000   150.000000             150\n",
       "unique           NaN          NaN           NaN          NaN               3\n",
       "top              NaN          NaN           NaN          NaN  Iris-virginica\n",
       "freq             NaN          NaN           NaN          NaN              50\n",
       "mean        5.843333     3.054000      3.758667     1.198667             NaN\n",
       "std         0.828066     0.433594      1.764420     0.763161             NaN\n",
       "min         4.300000     2.000000      1.000000     0.100000             NaN\n",
       "25%         5.100000     2.800000      1.600000     0.300000             NaN\n",
       "50%         5.800000     3.000000      4.350000     1.300000             NaN\n",
       "75%         6.400000     3.300000      5.100000     1.800000             NaN\n",
       "max         7.900000     4.400000      6.900000     2.500000             NaN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAFNCAYAAACuWnPfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEpGQgYAAFCy/w8AT04nguinnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bf82ab3588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plengths = df[\"petal_length\"]\n",
    "pwidths = df[\"petal_width\"]\n",
    "labels = df[\"label\"]\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.title(\"Petal width against petal length\")\n",
    "plt.scatter(plengths[labels==\"Iris-setosa\"], pwidths[labels==\"Iris-setosa\"], color = \"green\")\n",
    "plt.scatter(plengths[labels==\"Iris-versicolor\"], pwidths[labels==\"Iris-versicolor\"], color = \"red\")\n",
    "plt.scatter(plengths[labels==\"Iris-virginica\"], pwidths[labels==\"Iris-virginica\"], color = \"yellow\")\n",
    "plt.xlabel(\"Petal length (cm)\")\n",
    "plt.xlim(0, 8)\n",
    "plt.ylabel(\"Petal width (cm)\")\n",
    "plt.ylim(0, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Any observation you want to make?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul>\n",
    "    <li>For the hell of it, we'll fit two multinomial logistic regression models: \n",
    "        <ul>\n",
    "            <li>One-versus-rest</li>\n",
    "            <li>Cross entropy</li>\n",
    "        </ul>\n",
    "        and use a majority-class classifier (although we don't need to. Why?)\n",
    "    </li>\n",
    "    <li>Note: scikit-learn wants the class labels to be integers\n",
    "        <ul>\n",
    "            <li>In the <i>CS1109</i> dataset, they were already 0 or 1</li>\n",
    "            <li>In the Iris dataset, we must convert from \"Iris-setosa\", \"Iris-versicolor\" and \"Iris-virginica\" to 0, 1, 2</li>\n",
    "            <li>Note: we do <strong>not</strong> usually one-hot encode class labels</li>\n",
    "            <li>We use <code>LabelEncoder</code></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df[\"label\"].values\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y_encoded = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features we want to select\n",
    "features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "\n",
    "# Create the one-versus-rest pipeline\n",
    "ovr_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector(features)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"estimator\", LogisticRegression())\n",
    "    ])\n",
    "\n",
    "# Create the cross-entropy pipeline\n",
    "cent_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector(features)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"estimator\", LogisticRegression(multi_class=\"multinomial\", solver=\"newton-cg\"))\n",
    "    ])\n",
    "\n",
    "# Create the classifier\n",
    "maj_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector(features)),    \n",
    "        (\"estimator\", DummyClassifier(strategy = \"most_frequent\"))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-versus-Rest:  0.893333333333\n",
      "Cross entropy:  0.953333333333\n",
      "Majority-class classifier:  0.333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"One-versus-Rest: \", np.mean(cross_val_score(ovr_pipeline, df, y_encoded, scoring=\"accuracy\", cv=10)))\n",
    "print(\"Cross entropy: \", np.mean(cross_val_score(cent_pipeline, df, y_encoded, scoring=\"accuracy\", cv=10)))\n",
    "print(\"Majority-class classifier: \", np.mean(cross_val_score(maj_pipeline, df, y_encoded, scoring=\"accuracy\", cv=10)))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
