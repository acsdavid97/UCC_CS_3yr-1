{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Gradient Descent</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Gradient Descent for OLS Regression</h1>\n",
    "<ul>\n",
    "    <li>We saw the basic idea &mdash; now, the details</li>\n",
    "    <li>In fact, three variants:\n",
    "        <ul>\n",
    "            <li>Batch Gradient Descent</li>\n",
    "            <li>Stochastic Gradient Descent</li>\n",
    "            <li>Mini-batch Gradient Descent</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Partial Derivatives</h1>\n",
    "<ul>\n",
    "    <li>We need the <b>gradient</b> of the loss function with regards to each $\\v{\\beta}_j$\n",
    "        <ul>\n",
    "            <li>In other words, how much the loss will change if we change $\\v{\\beta}_j$ a little</li>\n",
    "            <li>With respect to a particular $\\v{\\beta}_j$, it is called the <b>partial derivative</b>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Without doing the calculus, the partial derivatives of $J(\\v{X}, \\v{y}, \\v{\\beta})$ with respect to\n",
    "        $\\v{\\beta}_j$ are\n",
    "        $$\\frac{\\partial J(\\v{X}, \\v{y}, \\v{\\beta})}{\\partial\\v{\\beta}_j} = \n",
    "          \\frac{1}{m}\\sum_{i=1}^m(\\v{x}^{(i)}\\v{\\beta} - \\v{y}^{(i)}) \\times \\v{x}_j^{(i)}$$\n",
    "    </li>\n",
    "    <li>The <b>gradient vector</b>, $\\nabla_{\\v{\\beta}}J(\\v{X}, \\v{y}, \\v{\\beta})$, is a vector\n",
    "        of each partial derivative:\n",
    "        $$\\nabla_{\\v{\\beta}}J(\\v{X}, \\v{y}, \\v{\\beta}) = \n",
    "            \\cv{\\frac{\\partial J(\\v{X}, \\v{y}, \\v{\\beta})}{\\partial\\v{\\beta}_0}\\\\ \n",
    "              \\frac{\\partial J(\\v{X}, \\v{y}, \\v{\\beta})}{\\partial\\v{\\beta}_1}\\\\ \n",
    "              \\vdots\\\\ \n",
    "              \\frac{\\partial J(\\v{X}, \\v{y}, \\v{\\beta})}{\\partial\\v{\\beta}_n}}\n",
    "        $$\n",
    "     </li>\n",
    "     <li>And there is a vectorized way to compute it:\n",
    "         $$\\nabla_{\\v{\\beta}}J(\\v{X}, \\v{y}, \\v{\\beta}) = \\frac{1}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y})$$\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Gradient Descent, Again</h1>\n",
    "<ul>\n",
    "    <li>Recap:\n",
    "        <ul>\n",
    "            <li>\n",
    "                 It starts with an initial guess for the values of the parameters \n",
    "            </li>\n",
    "            <li>\n",
    "                Then repeatedly:\n",
    "                <ul>\n",
    "                    <li>It updates the parameter values  &mdash; hopefully to reduce the loss\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "     </li>\n",
    "     <li>But now we know how to update the parameter values to reduce the loss:\n",
    "         <ul>\n",
    "             <li>Compute the gradient vector\n",
    "                 <ul>\n",
    "                     <li>But this points 'uphill' and we want to go 'downhill'</li>\n",
    "                     <li>And we want to make 'baby steps', so we use the learning rate, $\\alpha$, which\n",
    "                         is between 0 and 1\n",
    "                     </li>\n",
    "                 </ul>\n",
    "             </li>\n",
    "             <li>So subtract the $\\alpha$ times the gradient vector from $\\v{\\beta}$</li>\n",
    "         </ul>\n",
    "         $$\\v{\\beta} \\gets \\v{\\beta} - \\alpha\\nabla_{\\v{\\beta}}J(\\v{X}, \\v{y}, \\v{\\beta})$$\n",
    "         Or\n",
    "         $$\\v{\\beta} \\gets \\v{\\beta} - \\frac{\\alpha}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y})$$\n",
    "     </li>\n",
    "     <li>(BTW, this is vectorized. Naive loop implementations are wrong: they lose the\n",
    "         <em>simultaneous</em> update of the $\\v{\\beta}_j$)\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Batch Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li>Pseudocode:\n",
    "        <ul style=\"background: lightgrey; list-style: none\">\n",
    "            <li>initialize $\\v{\\beta}$ randomly\n",
    "            <li>\n",
    "                repeat until convergence\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        $\\v{\\beta} \\gets \\v{\\beta} - \\frac{\\alpha}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y})$\n",
    "                    </li>\n",
    "                </ul>\n",
    "             </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Why is it called <em>Batch</em> Gradient Descent?\n",
    "        <ul>\n",
    "            <li>The update involves a calculation over the <em>entire</em> training set $\\v{X}$\n",
    "                on every iteration\n",
    "            </li>\n",
    "            <li>This can be slow for large training sets</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Batch Gradient Descent in numpy</h2>\n",
    "<ul>\n",
    "    <li>For the hell of it, let's implement it ourselves</li>\n",
    "    <li>(We'll be naughty: we'll train on the whole dataset)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function for OLS regression (assumes X contains all 1s in its first column)\n",
    "def J(X, y, beta):\n",
    "    return np.mean((X.dot(beta) - y) ** 2) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gradient_descent_for_ols_linear_regression(X, y, alpha, num_iterations):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    beta = np.random.randn(n) \n",
    "    Jvals = np.zeros(num_iterations)\n",
    "    \n",
    "    for iter in range(num_iterations):\n",
    "        beta -= (1.0 * alpha / m) * X.T.dot(X.dot(beta) - y)\n",
    "        Jvals[iter] = J(X, y, beta)\n",
    " \n",
    "    return beta, Jvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 274.72463768,  140.91111676,   10.20466646,   -0.78810177])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas to read the CSV file\n",
    "df = pd.read_csv(\"datasets/dataset_corkA.csv\")\n",
    "\n",
    "# Get the feature-values and the target values \n",
    "X_without_dummy_unscaled = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "# Scale it\n",
    "scaler = StandardScaler()\n",
    "X_without_dummy = scaler.fit_transform(X_without_dummy_unscaled)\n",
    "\n",
    "# Add the extra column to X\n",
    "X = add_dummy_feature(X_without_dummy)\n",
    "\n",
    "# Run the Batch Gradient Descent\n",
    "beta, Jvals = batch_gradient_descent_for_ols_linear_regression(X, y, alpha = 0.03, num_iterations = 1000)\n",
    "\n",
    "# Display beta\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Bear in mind that the coefficients it finds are on the scaled data</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>It's a good idea to plot the values of the loss function against the number of iterations\n",
    "    </li>\n",
    "    <li>If its value ever increases, then\n",
    "        <ul>\n",
    "            <li>\n",
    "                the code might be incorrect (I think it's OK!)\n",
    "            </li>\n",
    "            <li>\n",
    "                the value of $\\alpha$ is too big and is causing divergence\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAGFCAYAAACothrZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJKkzg4QkSerMICFJkjozSEiSpM7+F5IVBp/t2yUAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2965aca99b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.title(\"$J$ during learning\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.xlim(1, Jvals.size)\n",
    "plt.ylabel(\"$J$\")\n",
    "plt.ylim(3500, 50000)\n",
    "xvals = np.linspace(1, Jvals.size, Jvals.size)\n",
    "plt.scatter(xvals, Jvals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>The algorithm gives us the problem of choosing the number of iterations</li>\n",
    "    <li>An alternative is to use a very large number of iterations but exit when the gradient vector\n",
    "        becomes tiny:\n",
    "        <ul>\n",
    "            <li>when its norm becomes smaller than <b>tolerance</b>, $\\eta$</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Try it without scaling:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: RuntimeWarning: overflow encountered in square\n",
      "  app.launch_new_instance()\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ nan,  nan,  nan,  nan])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature-values and the target values \n",
    "X_without_dummy = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "# Add the extra column to X\n",
    "X = add_dummy_feature(X_without_dummy)\n",
    "\n",
    "# Run the Batch Gradient Descent\n",
    "beta, Jvals = batch_gradient_descent_for_ols_linear_regression(X, y, alpha = 0.03, num_iterations = 4000)\n",
    "\n",
    "# Display beta\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>How can you get it to work?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>\n",
    "        Some people suggest a variant of Batch Gradient Descent in which the value of $\\alpha$ is decreased\n",
    "        over time, i.e. its value in later iterations is smaller\n",
    "        <ul>\n",
    "            <li>Why do they suggest this? </li>\n",
    "            <li>And why isn't it necessary?\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>(But, we'll revisit this idea in Stochastic Gradient Descent)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Stochastic Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li>As we saw, Batch Gradient Descent can be slow on large training sets</li>\n",
    "    <li><b>Stochastic Gradient Descent (SGD)</b>:\n",
    "        <ul>\n",
    "            <li>On each iteration, it picks just <em>one</em> training example $\\v{x}$ at random and computes \n",
    "                the gradients on just that\n",
    "                one example\n",
    "                $$\\v{\\beta} \\gets \\v{\\beta} - \\alpha\\v{x}^T(\\v{x}\\v{\\beta} - \\v{y})$$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This gives huge speed-up</li>\n",
    "    <li>It enables us to train on huge training sets since only one example needs to be in memory in each iteration</li>\n",
    "    <li>But, because it is stochastic (the randomness), the loss will not necessarily decrease on each iteration:\n",
    "        <ul>\n",
    "            <li><em>On average</em>, the loss decreases, but in any one iteration, loss may go up or down</li>\n",
    "            <li>Eventually, it will get close to the minimum, but it will continue to go up and down a bit\n",
    "                <ul>\n",
    "                    <li>So, once you stop it, the $\\v{\\beta}$ will be close to the best, but not necessarily optimal</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Ironically, if you have a local minimum (which, with OLS regression, we don't), SGD might even escape the\n",
    "                local minimum, and might even get to the global minimum\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Simulated Annealing</h2>\n",
    "<ul>\n",
    "    <li>As we discussed, SGD does not settle at the minimum</li>\n",
    "    <li>One solution is to gradually reduce the learning rate\n",
    "        <ul>\n",
    "            <li>Updates start out 'large' so you make progress and can escape local minima</li>\n",
    "            <li>But, over time, updates get smaller, allowing SGD to settle at the global minimum</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The function that determines how to reduce the learning rate is called the <b>learning schedule</b>\n",
    "        <ul>\n",
    "            <li>Reduce it too quickly and you may get stuck in a local  minimum or en route to the global minimum</li>\n",
    "            <li>Reduce it too slowly and you may bounce around a lot and, if stopped after too few iterations, may end up\n",
    "                with a suboptimal solution\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>SGD in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>The <code>fit</code> method of scikit-learn's <code>SGDRegressor</code> class is doing\n",
    "        what we have described:\n",
    "        <ul>\n",
    "            <li>You must scale the features but it inserts the extra column of 1s</li>\n",
    "            <li>You can supply a <code>learning_rate</code> and lots of other things\n",
    "                (in the code below, we'll just use the defaults)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>(In the code below, we'll be naughty: we'll train on the whole dataset)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,\n",
       "       random_state=None, shuffle=True, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas to read the CSV file\n",
    "df = pd.read_csv(\"datasets/dataset_corkA.csv\")\n",
    "\n",
    "# Get the feature-values and the target values \n",
    "X_unscaled = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "# Scale it\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_unscaled)\n",
    "\n",
    "# Create the SGDRegressor and fit the model\n",
    "sgd = SGDRegressor()\n",
    "sgd.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>SGD in numpy</h2>\n",
    "<ul>\n",
    "    <li>For the hell of it, let's implement a simple version ourselves</li>\n",
    "    <li>(Again, we'll be naughty: we'll train on the whole dataset)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_for_ols_linear_regression(X, y, alpha, num_epochs):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    beta = np.random.randn(n) \n",
    "    Jvals = np.zeros(num_epochs * m)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(m):\n",
    "            rand_idx = np.random.randint(m)\n",
    "            xi = X[rand_idx:rand_idx + 1]\n",
    "            yi = y[rand_idx:rand_idx + 1]\n",
    "            beta -= alpha * xi.T.dot(xi.dot(beta) - yi)\n",
    "            Jvals[epoch * m + i] = J(X, y, beta)\n",
    " \n",
    "    return beta, Jvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 275.78125157,  121.2149965 ,   36.83866659,  -16.55484846])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas to read the CSV file\n",
    "df = pd.read_csv(\"datasets/dataset_corkA.csv\")\n",
    "\n",
    "# Get the feature-values and the target values \n",
    "X_without_dummy_unscaled = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "# Scale it\n",
    "scaler = StandardScaler()\n",
    "X_without_dummy = scaler.fit_transform(X_without_dummy_unscaled)\n",
    "\n",
    "# Add the extra column to X\n",
    "X = add_dummy_feature(X_without_dummy)\n",
    "\n",
    "# Run the Stochastic Gradient Descent\n",
    "beta, Jvals = stochastic_gradient_descent_for_ols_linear_regression(X, y, alpha = 0.03, num_epochs = 50)\n",
    "\n",
    "# Display beta\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAGFCAYAAAB3+GDGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAACiCBIAAEDU/wcNmQHgH+FJkwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2965ace5e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.title(\"$J$ during learning\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.xlim(1, Jvals.size)\n",
    "plt.ylabel(\"$J$\")\n",
    "plt.ylim(3500, 50000)\n",
    "xvals = np.linspace(1, Jvals.size, Jvals.size)\n",
    "plt.scatter(xvals, Jvals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Quite a bumpy ride!</li>\n",
    "    <li>So, let's try simuated annealingL</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_schedule(t):\n",
    "    return 5 / (t + 50)\n",
    "    \n",
    "def stochastic_gradient_descent_for_ols_linear_regression(X, y, num_epochs):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    beta = np.random.randn(n) \n",
    "    Jvals = np.zeros(num_epochs * m)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(m):\n",
    "            rand_idx = np.random.randint(m)\n",
    "            xi = X[rand_idx:rand_idx + 1]\n",
    "            yi = y[rand_idx:rand_idx + 1]\n",
    "            alpha = learning_schedule(epoch * m + i)\n",
    "            beta -= alpha * xi.T.dot(xi.dot(beta) - yi)\n",
    "            Jvals[epoch * m + i] = J(X, y, beta)\n",
    " \n",
    "    return beta, Jvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.75318942e+02,   1.38082587e+02,   1.36263924e+01,\n",
       "         2.07068421e-01])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas to read the CSV file\n",
    "df = pd.read_csv(\"datasets/dataset_corkA.csv\")\n",
    "\n",
    "# Get the feature-values and the target values \n",
    "X_without_dummy_unscaled = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "# Scale it\n",
    "scaler = StandardScaler()\n",
    "X_without_dummy = scaler.fit_transform(X_without_dummy_unscaled)\n",
    "\n",
    "# Add the extra column to X\n",
    "X = add_dummy_feature(X_without_dummy)\n",
    "\n",
    "# Run the Stochastic Gradient Descent\n",
    "beta, Jvals = stochastic_gradient_descent_for_ols_linear_regression(X, y, num_epochs = 50)\n",
    "\n",
    "# Display beta\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAGFCAYAAAB3+GDGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDAmSJKnT/wcsdN74IrujaAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2965bffc828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.title(\"$J$ during learning\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.xlim(1, Jvals.size)\n",
    "plt.ylabel(\"$J$\")\n",
    "plt.ylim(3500, 50000)\n",
    "xvals = np.linspace(1, Jvals.size, Jvals.size)\n",
    "plt.scatter(xvals, Jvals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Mini-Batch Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li>Batch Gradient Descent computed gradients from the full training set</li>\n",
    "    <li>Stochastic Gradient Descent computed gradients from just one example</li>\n",
    "    <li>Mini-Batch Gradient Descent lies between the two:\n",
    "        <ul>\n",
    "            <li>It computes gradients from a small randomly-selected subset of the training set, called a\n",
    "                <b>mini-batch</b>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Since it lies between the two:\n",
    "        <ul>\n",
    "            <li>It may bounce less and get closer to the global minimum than SGD\n",
    "                <ul>\n",
    "                    <li>Although both of them can reach the global minimum with a good learning schedule</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>But it may be harder to escape local minima, if you have them (which, for OLS, we don't)</li>\n",
    "            <li>And its time and memory costs lie between the two</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Normal Equation versus Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li>Efficiency/scaling-up\n",
    "        <ul>\n",
    "            <li>Normal Equation \n",
    "                <ul>\n",
    "                    <li>is linear in $m$, so can handle large training sets efficiently if they fit into\n",
    "                        main memory\n",
    "                    </li>\n",
    "                    <li>but it has to compute the inverse (or psueudo-inverse) of a $n \\times n$ matrix, which takes\n",
    "                        time between quadratic and cubic in $n$, and so is only feasible for smallish $n$ (up to\n",
    "                        a few thousand)\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Gradient Descent\n",
    "                <ul>\n",
    "                    <li>SGD scales really well to huge $m$</li>\n",
    "                    <li>And all three Gradient Descent methods can handle huge $n$ (even 100s of 1000s)</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Finding the global minimum for OLS regression\n",
    "        <ul>\n",
    "            <li>Normal Equation: guaranteed to find the global minimum</li>\n",
    "            <li>Gradient Descent: all a bit dependent on number of iterations, learning rate, learning schedule</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Feature scaling:\n",
    "        <ul>\n",
    "            <li>Normal Equation: scaling is not needed\n",
    "                <ul>\n",
    "                    <li>(In fact, I find that scikit-learn's <code>LinearRegression</code> class produces weird\n",
    "                        results if I do any scaling. I don't know why. So don't do it!)\n",
    "                    </li>\n",
    "                </ul>   \n",
    "            </li>\n",
    "            <li>Gradient Descent: scaling <em>is</em> needed</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Finally, Gradient Descent is a general method, whereas the Normal Equation is only for OLS regression</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logicstic Regression</h1>\n",
    "<ul>\n",
    "    <li>So what about classification using logistic regression?</li>\n",
    "    <li>We have a different loss function (cross entropy)\n",
    "        <ul>\n",
    "            <li>Happily, it is convex</li>\n",
    "            <li>But there is no equivalent to the Normal Equation, so we <em>must</em> use Gradient Descent</li>\n",
    "            <li>Not that it matters, but here is the partial derivative of its loss function with respect to $\\v{\\beta}_i$\n",
    "                (binary classification)\n",
    "                $$\\frac{\\partial J}{\\partial\\v{\\beta}_j} =\n",
    "                  \\frac{1}{m}\\sum_{i=1}^m(\\v{x}^{(i)})\\v{\\beta} - \\v{y}^{(i)}) \\times \\v{x}_j^{(i)}$$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>scikit-learn has the class <code>LogisticRegression</code>, but also <code>SGDClassifier</code> if you\n",
    "        want more control\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>After Christmas&hellip;</h1>\n",
    "<ul>\n",
    "    <li>Here endeth CS4618</li>\n",
    "    <li>What will we do in CS4619?\n",
    "        <ul>\n",
    "            <li>We will study some more complex models (i.e. non-linear ones)</li>\n",
    "            <li>We will study underfitting and overfitting, and solutions to these</li>\n",
    "            <li>This will lead into Neural Networks</li>\n",
    "            <li>From there, we will study so-called Deep Learning for regression and classification, including\n",
    "                for images\n",
    "            </li>\n",
    "            <li>We will generalize to problems such as sequence to vector, vector to sequence and sequence to sequences\n",
    "                such as machine translation, speech recognition,&hellip;\n",
    "            </li>\n",
    "            <li>We will reviste Reinforcement Learning</li>\n",
    "            <li>We will consider knowledge representation and reasoning</li>\n",
    "        </ul>\n",
    "        It'll be tough but brilliant\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
