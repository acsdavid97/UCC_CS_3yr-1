{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Clustering: Introduction</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Class, for use in pipelines, to select certain columns from a DataFrame and convert to a numpy array\n",
    "# From A. Geron: Hands-On Machine Learning with Scikit-Learn & TensorFlow, O'Reilly, 2017\n",
    "# Modified by Derek Bridge to allow for casting in the same ways as pandas.DataFrame.astype\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype=None):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_selected = X[self.attribute_names]\n",
    "        if self.dtype:\n",
    "            return X_selected.astype(self.dtype).values\n",
    "        return X_selected.values\n",
    "    \n",
    "# Class, for use in pipelines, to binarize nominal-valued features (while avoiding the dummy variabe trap)\n",
    "# By Derek Bridge, 2017\n",
    "class FeatureBinarizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_values):\n",
    "        self.features_values = features_values\n",
    "        self.num_features = len(features_values)\n",
    "        self.labelencodings = [LabelEncoder().fit(feature_values) for feature_values in features_values]\n",
    "        self.onehotencoder = OneHotEncoder(sparse=False,\n",
    "            n_values=[len(feature_values) for feature_values in features_values])\n",
    "        self.last_indexes = np.cumsum([len(feature_values) - 1 for feature_values in self.features_values])\n",
    "    def fit(self, X, y=None):\n",
    "        for i in range(0, self.num_features):\n",
    "            X[:, i] = self.labelencodings[i].transform(X[:, i])\n",
    "        return self.onehotencoder.fit(X)\n",
    "    def transform(self, X, y=None):\n",
    "        for i in range(0, self.num_features):\n",
    "            X[:, i] = self.labelencodings[i].transform(X[:, i])\n",
    "        onehotencoded = self.onehotencoder.transform(X)\n",
    "        return np.delete(onehotencoded, self.last_indexes, axis=1)\n",
    "    def fit_transform(self, X, y=None):\n",
    "        onehotencoded = self.fit(X).transform(X)\n",
    "        return np.delete(onehotencoded, self.last_indexes, axis=1)\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"features_values\" : self.features_values}\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            self.setattr(parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Clustering</h1>\n",
    "<ul>\n",
    "    <li><b>Clustering</b> is the process of grouping objects according to some distance measure</li>\n",
    "    <li>The goals:\n",
    "        <ul>\n",
    "            <li>two objects in the same cluster are a small distance from each other</li>\n",
    "            <li>two objects in different clusters are a large distance from each other</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>E.g. how would you cluster these dogs? \n",
    "        <img style=\"float: right\" src=\"images/13_dogs.jpg\" />\n",
    "    </li>\n",
    "    <li>Applications:\n",
    "        <ul>\n",
    "            <li>Genetics: discovering groups of genes that express themselves in similar ways</li>\n",
    "            <li>Marketing: segmenting customers for targeted advertising or to drive new product development</li>\n",
    "            <li>Social network analysis: discovering communities in social networks</li>\n",
    "            <li>Social sciences: analysing populations based on demographics, behaviour, etc</li>\n",
    "            <li>Genetic algorithms: identifying population niches in an effort to maintain diversity</li>\n",
    "            <li>&hellip;</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Note: Clustering algorithms assign the objects to groups, but they are typically not capable\n",
    "        of giving meaningful labels (names) to the groups\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Clustering algorithms</h1>\n",
    "<ul>\n",
    "    <li>There are many, many algorithms, falling roughly into two kinds:\n",
    "        <ul>\n",
    "            <li><b>Point-assignment algorithms</b>: \n",
    "                <ul>\n",
    "                    <li>objects are initially assigned to clusters, e.g., arbitrarily</li>\n",
    "                    <li>then, repeatedly, each object is re-considered: it may be assigned to a cluster to \n",
    "                        which it is more closely related\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li><b>Hierarchical algorithms</b>: produce a tree of clusters\n",
    "                <ul>\n",
    "                    <li><b>Agglomerative algorithms</b> ('bottom-up'):\n",
    "                        <ul>\n",
    "                            <li>each object starts in a 'cluster' on its own; </li>\n",
    "                            <li>then, recursively, pairs of clusters are merged to form a parent cluster</li>\n",
    "                        </ul>\n",
    "                    </li>\n",
    "                    <li><b>Divisive algorithms</b> ('top-down'):\n",
    "                        <ul>\n",
    "                            <li>all objects start in a single cluster;</li>\n",
    "                            <li>then, recursively, a cluster is split into child clusters</li>\n",
    "                        </ul>\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>There are lots of other ways of distinguishing clustering algorithms from each other, e.g.\n",
    "        <ul>\n",
    "            <li>partitioning: must every object belong to exactly one cluster, or may some objects belong to more\n",
    "                than one cluster and may some objects belong to no cluster?\n",
    "            </li>\n",
    "            <li>hard vs. soft: is membership of a cluster Boolean (an object belongs to a cluster or it does not) \n",
    "                or is it fuzzy (there are degrees of membership, e.g. it is 0.8 true that this object belongs \n",
    "                to this cluster) or probabilistic\n",
    "            </li>\n",
    "            <li>whether they only work for certain distance measures (e.g. Euclidean, Manhattan, Chebyshev) and not\n",
    "                for others (e.g. cosine)\n",
    "            </li>\n",
    "            <li>whether they assume a dataset that fits into main memory or whether they scale to larger\n",
    "                datasets\n",
    "            </li>\n",
    "            <li>whether they assume all the data is available up-front, or whether they assume it arrives\n",
    "                over time\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We'll study two of the simpler algorithms: one point-assignment and one hierarchical</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>$k$-Means Clustering</h1>\n",
    "<ul>\n",
    "    <li>The <b>$k$-means algorithm</b> is the best-known <em>point-assignment algorithm</em>\n",
    "        <ul>\n",
    "            <li>E.g. the <code>KMeans</code> class in scikit-learn</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>It assumes that you know the number of clusters, $k$, in advance</li>\n",
    "    <li>Given a dataset of examples (as vectors) $\\v{X}$ it returns a <em>partition</em> of $\\v{X}$ into\n",
    "        $k$ subsets\n",
    "    </li>\n",
    "    <li>Key concept: the <b>centroid</b> of a cluster\n",
    "        <ul>\n",
    "            <li>the <em>mean</em> of the examples in that cluster, i.e. the mean of each feature\n",
    "            </li>\n",
    "        </ul>\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Centroids</h1>\n",
    "<ul>\n",
    "    <li>Class exercise: What are the centroids of these clusters?\n",
    "        <ol>\n",
    "            <li>$\\Set{\\cv{1\\\\1\\\\1}, \\cv{2\\\\4\\\\6}, \\cv{3\\\\7\\\\11}}$\n",
    "            <li>$\\Set{\\cv{4\\\\3\\\\7}}$</li>\n",
    "            <li>$\\Set{\\cv{2\\\\3}, \\cv{4\\\\2}}$</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>Observations:\n",
    "        <ul>\n",
    "            <li>The centroid of a cluster that contains just one example is the example itself</li>\n",
    "            <li>The centroid of a cluster that contains more than one example may not even be one of the\n",
    "                examples in the cluster\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>$k$-Means Algorithm</h1>\n",
    "<ul>\n",
    "    <li>It starts by choosing $k$ examples from $\\v{X}$ to be the initial centroids, e.g. randomly</li>\n",
    "    <li>Then, repeatedly,\n",
    "        <ul>\n",
    "            <li><b>Assignment step</b>: Each example $\\v{x} \\in \\v{X}$ is assigned to one of the clusters: \n",
    "                the one whose centroid is closest to $\\v{x}$\n",
    "            </li>\n",
    "            <li><b>Update step</b>: It re-computes the centroids of the clusters</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Toy Example, $k = 2$</h1>\n",
    "<table>\n",
    "    <tr style=\"border-bottom: 0\">\n",
    "        <td style=\"border-bottom: 0\">Dataset $\\v{X}$</td><td style=\"border-bottom: 0\">Random centroids</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-top: 0\">\n",
    "        <td style=\"border-top: 0\"><img src=\"images/13_km1.png\" /></td>\n",
    "        <td style=\"border-top: 0\"><img src=\"images/13_km2.png\" /></td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 0\">\n",
    "        <td style=\"border-bottom: 0\">Assignment step</td><td style=\"border-bottom: 0\">Update step</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-top: 0\">\n",
    "        <td style=\"border-top: 0\"><img src=\"images/13_km3.png\" /></td>\n",
    "        <td style=\"border-top: 0\"><img src=\"images/13_km4.png\" /></td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 0\">\n",
    "        <td style=\"border-bottom: 0\">Assignment step</td><td style=\"border-bottom: 0\">Update step</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-top: 0\">\n",
    "        <td style=\"border-top: 0\"><img src=\"images/13_km5.png\" /></td>\n",
    "        <td style=\"border-top: 0\"><img src=\"images/13_km6.png\" /></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>When to stop?</h1>\n",
    "<ul>\n",
    "    <li>If you run it for enough iterations, there <em>usually</em> comes a point when\n",
    "        <ul>\n",
    "            <li>In the update step, the centroids don't change</li>\n",
    "            <li>Hence, in the assignment step, the clustering doesn't change</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But there is a small risk that it <em>never</em> happens and that the algorithm oscillates between two or more\n",
    "        equaly good solutions\n",
    "    </li>\n",
    "    <li>Therefore, most implementations have a maximum number of iterations (<code>max_iter</code>\n",
    "        in scikit-learn)\n",
    "    </li>\n",
    "    <li>They might stop earlier, when the algorithm converges &mdash; next slide\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Inertia and Convergence</h1>\n",
    "<ul>\n",
    "    <li>What is $k$-means trying to achieve?\n",
    "        <ul>\n",
    "            <li>A clustering that <b>minimizes inertia</b>: the within-cluster sum of distances</li>\n",
    "            <li>I.e. the sum of the distances from each $\\v{x} \\in \\v{X}$ to its centroid is as low as possible\n",
    "            </li>\n",
    "        </ul>\n",
    "        (Advanced: The algorithm is more correctly formalized as trying to minimise the within-cluster\n",
    "        sum of squares of disstances, but with Euclidean distance, the best clustering is the same)\n",
    "    </li>\n",
    "    <li>If you run it for enough iterations, it will <b>converge</b>\n",
    "        <ul>\n",
    "            <li>I.e. the inertia will remain unchanged between iterations</li>\n",
    "        </ul>\n",
    "        The algorithm can stop at this point\n",
    "    </li>\n",
    "    <li>Most implementations have a tolerance (<code>tol</code> in scikit-learn):\n",
    "        <ul>\n",
    "            <li>They stop when the change in inertia falls below the tolerance, rather than waiting\n",
    "                for zero change\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Local and Global Minima</h1>\n",
    "<ul>\n",
    "    <li>Even if the algorithm converges (no improvement in inertia), the clustering it converges on\n",
    "        might not be the <b>global minimum</b> (the one with lowest possible inertia)</li>\n",
    "    <li>$k$-means produces different clustering depending on the choice of the initial $k$ centroids</li>\n",
    "    <li>For a given set of initial centroids, the clustering it converges on might be a <b>local minimum</b>:\n",
    "        <ul>\n",
    "            <li>For these initial centroids, no better clustering can be found, but it's not\n",
    "                the very best clustering possible\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Class exercise. Here, $\\v{X}$ contains four examples at the corners of a rectangle:\n",
    "        <figure>\n",
    "            <img src=\"images/13_minima.png\" />\n",
    "        </figure>\n",
    "        <ul>\n",
    "            <li>For $k=2$, choose initial centroids that result in a global minimum</li>\n",
    "            <li>And choose $k=2$ centroids that give a local minimum</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Let's look at ways of reducing the problem&hellip;</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Avoiding local minima: re-running</h2>\n",
    "<ul>\n",
    "    <li>The obvious solution is to run $k$-means multiple times (with different initial centroids) and\n",
    "        return the clustering that has the lowest inertia\n",
    "    </li>\n",
    "    <li>No guarantee of finding the global minimum this way but likely to be better</li>\n",
    "    <li>E.g. scikit-learn the number of runs (<code>n_init</code>) is 10, by default</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Avoiding local minima: better initial centroids</h2>\n",
    "<ul>\n",
    "    <li>Choosing the initial $k$ centroids at random from $\\v{X}$ has problems:\n",
    "        <ul>\n",
    "            <li>The algorithm can return different clusters for $\\v{X}$ each time it is run\n",
    "            </li>\n",
    "            <li>The clustering it returns may be a local minima</li>\n",
    "            <li>A poor choice can increase the number of iterations needed for convergence\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>There are many alternatives to choosing wholly randomly, e.g.:\n",
    "        <ul>\n",
    "            <li>insert into $\\mathit{Centroids}$ an example $\\v{x} \\in \\v{X}$,\n",
    "                chosen at random with uniform probability\n",
    "            </li>\n",
    "            <li>while $|\\mathit{Centroids}| < k$\n",
    "                <ul>\n",
    "                    <li>insert into $\\mathit{Centroids}$ a different example $\\v{x} \\in \\v{X}$,\n",
    "                        chosen with probability proportional to $(\\min_{\\v{x}' \\in \\mathit{Centroids}}\\dist(\\v{x}, \\v{x}'))^2$\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "     </li>\n",
    "     <li>$k$-means++ is the name of the $k$-means algorithm when using the above method\n",
    "         <ul>\n",
    "             <li>it still has randomness, so it still suffers from the problems above, but typically less so</li>\n",
    "         </ul>\n",
    "     </li>\n",
    "     <li>In scikit-learn, the <code>init</code> parameter can have values <code>‘k-means++’</code> (default)\n",
    "         or <code>‘random'</code>\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>$k$-means clustering: discussion</h1>\n",
    "<ul>\n",
    "    <li>$k$-means can work well\n",
    "        <ul>\n",
    "            <li>but not so much in the presence of outliers or when the natural clusters are elongated or \n",
    "                irregular shapes\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The curse of dimensionality may be relevant\n",
    "        <ul>\n",
    "            <li>You might want to include dimensionality reduction such as PCA in your pipeline</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The algorithm mostly scales well to larger data\n",
    "        <ul>\n",
    "            <li>There are variants for speed-up, e.g. <code>MiniBatchKMeans</code> in scikit-learn\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>There is the problem of choosing $k$ in advance\n",
    "        <ul>\n",
    "            <li>Why does it not make sense to run it with all $k$ in $[2,m]$ and choose the clustering\n",
    "                with lowest inertia?\n",
    "            </li>\n",
    "            <li>There are point-assignment algorithms that do not require you to choose $k$ in advance</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>$k$-Means in scikit-learn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"datasets/dataset_corkA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features we want to select\n",
    "numeric_features = [\"flarea\", \"bdrms\", \"bthrms\", \"floors\"]\n",
    "nominal_features = [\"type\", \"devment\", \"ber\", \"location\"]\n",
    "\n",
    "# Create the pipelines\n",
    "numeric_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector(numeric_features)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "nominal_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector(nominal_features)), \n",
    "        (\"binarizer\", FeatureBinarizer([df[feature].unique() for feature in nominal_features]))])\n",
    "\n",
    "pipeline = Pipeline([(\"union\", FeatureUnion([(\"numeric_pipeline\", numeric_pipeline), \n",
    "                                             (\"nominal_pipeline\", nominal_pipeline)]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "pipeline.fit(df)\n",
    "X = pipeline.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the clustering object\n",
    "\n",
    "k = 2\n",
    "kmeans = KMeans(n_clusters=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run it\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983.77017887121383"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In case you're interested, you can see the final inertia\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.31820283e-01,  -3.64013409e-01,  -3.48297304e-01,\n",
       "         -1.29320334e-01,   1.65605096e-01,   1.46496815e-01,\n",
       "          3.75796178e-01,   9.80891720e-01,   6.36942675e-03,\n",
       "          1.91082803e-02,   1.01910828e-01,   7.64331210e-02,\n",
       "          7.00636943e-02,   9.55414013e-02,   1.01910828e-01,\n",
       "          1.27388535e-01,   8.28025478e-02,   8.28025478e-02,\n",
       "          1.33757962e-01,   3.18471338e-02,   1.91082803e-02,\n",
       "          1.91082803e-02,   2.54777070e-02,  -1.30104261e-17,\n",
       "          1.91082803e-02,   3.18471338e-02,   1.01910828e-01,\n",
       "          6.36942675e-03,   2.42038217e-01,  -1.30104261e-17,\n",
       "          1.27388535e-02,   1.08280255e-01,  -1.30104261e-17,\n",
       "          2.54777070e-02,   6.36942675e-03,   7.64331210e-02,\n",
       "          1.91082803e-02,   2.54777070e-02,   6.36942675e-03,\n",
       "         -2.60208521e-17,   1.91082803e-02,   6.36942675e-03,\n",
       "         -2.42861287e-17,   1.91082803e-02,   2.54777070e-02,\n",
       "          5.09554140e-02,   1.27388535e-02,   6.36942675e-03,\n",
       "          1.27388535e-02,   2.54777070e-02,   1.27388535e-02,\n",
       "         -5.20417043e-17,   6.36942675e-03,   1.91082803e-02],\n",
       "       [  1.35591569e+00,   1.14300210e+00,   1.09365353e+00,\n",
       "          4.06065849e-01,   2.00000000e-02,   7.60000000e-01,\n",
       "          1.20000000e-01,   1.00000000e+00,   4.33680869e-18,\n",
       "          1.40000000e-01,   8.00000000e-02,   8.00000000e-02,\n",
       "          1.20000000e-01,   1.60000000e-01,   1.00000000e-01,\n",
       "          6.00000000e-02,   1.00000000e-01,   6.00000000e-02,\n",
       "          8.00000000e-02,  -1.73472348e-17,   1.21430643e-17,\n",
       "          1.21430643e-17,   4.00000000e-02,   2.00000000e-02,\n",
       "          4.00000000e-02,   2.00000000e-02,   1.40000000e-01,\n",
       "          2.00000000e-02,   4.00000000e-02,   2.00000000e-02,\n",
       "          8.67361738e-18,   2.20000000e-01,   2.00000000e-02,\n",
       "          1.73472348e-17,   4.33680869e-18,   2.00000000e-02,\n",
       "          1.21430643e-17,   1.73472348e-17,   4.33680869e-18,\n",
       "          4.00000000e-02,   1.21430643e-17,   2.00000000e-02,\n",
       "          6.00000000e-02,   1.21430643e-17,   8.00000000e-02,\n",
       "          8.00000000e-02,   8.67361738e-18,   2.00000000e-02,\n",
       "          8.67361738e-18,   1.73472348e-17,   8.67361738e-18,\n",
       "          8.00000000e-02,   4.33680869e-18,   2.00000000e-02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...and even the vectors of the final centroids\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The clusters have been labeled (numbered) from 0...(k-1)\n",
    "# We can see the labels of each example in the dataset\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's hack up a function that helps us look at a few examples from each cluster\n",
    "def inspect_clusters(alg, df, k, features_to_show, how_many_to_show=None):\n",
    "    for i in range(0, k):\n",
    "        print(\"A few examples from cluster \", i)\n",
    "        indexes = alg.labels_ == i\n",
    "        max_available = indexes.sum()\n",
    "        print(df.ix[indexes, features_to_show]\n",
    "                   [:max_available if not how_many_to_show else min(how_many_to_show, max_available)])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few examples from cluster  0\n",
      "   flarea  bdrms  bthrms  floors           type     devment ber  location\n",
      "1    83.6      3       1       1       Detached  SecondHand  D2  Glanmire\n",
      "2    97.5      3       2       2  Semi-detached  SecondHand  D1  Glanmire\n",
      "4   118.7      3       2       2  Semi-detached  SecondHand  E2   Douglas\n",
      "\n",
      "A few examples from cluster  1\n",
      "   flarea  bdrms  bthrms  floors           type     devment ber      location\n",
      "0   497.0      4       5       2       Detached  SecondHand  B2  Carrigrohane\n",
      "3   158.0      5       2       2  Semi-detached  SecondHand  C3       Douglas\n",
      "5   170.0      4       4       2       Detached  SecondHand  D1       Douglas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show 3 examples from each cluster for KMeans with random initialization\n",
    "inspect_clusters(kmeans, df, k, numeric_features + nominal_features, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Go back and try with a different value for $k$</li>\n",
    "    <li>But eye-balling examples from the clusters is not a reliable way of judging the quality of the clustering</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Evaluating clustering, part 1</h1>\n",
    "<ul>\n",
    "    <li>Suppose someone has already done a manual clustering of the dataset ('ground truth'):\n",
    "        <ul>\n",
    "            <li>Then you can compare the output of the algorithm with the ground truth</li>\n",
    "            <li>Discussed in next lecture</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Suppose you don't have a ground truth (much more typical!):\n",
    "        <ul>\n",
    "            <li><b>Silhouette Coefficient</b> is one of several ways of scoring clustering quality:\n",
    "                <ul>\n",
    "                    <li>For each example $\\v{x} \\in \\v{X}$, compute \n",
    "                        $$\\frac{b - a}{max(a, b)}$$\n",
    "                        where $a$ is the mean distance between $\\v{x}$ and all other examples in the same cluster,\n",
    "                        $b$ is the mean distance between $\\v{x}$ and all examples in the next nearest cluster\n",
    "                    </li>\n",
    "                    <li>The Silhouette Coefficient is the mean of all of these</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                Its values lies in $[-1,1]$:\n",
    "                <ul>\n",
    "                    <li>Positive values suggest examples are in their correct clusters</li>\n",
    "                    <li>Values near 0 indicate clusters that are not well separated</li>\n",
    "                    <li>Negative values suggest examples are in the wrong clusters</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25888899875904836"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(X, kmeans.labels_, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
