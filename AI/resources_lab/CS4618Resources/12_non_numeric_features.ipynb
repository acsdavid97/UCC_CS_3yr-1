{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Non-Numeric Features</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Class, for use in pipelines, to select certain columns from a DataFrame and convert to a numpy array\n",
    "# From A. Geron: Hands-On Machine Learning with Scikit-Learn & TensorFlow, O'Reilly, 2017\n",
    "# Modified by Derek Bridge to allow for casting in the same ways as pandas.DataFrame.astype\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype=None):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_selected = X[self.attribute_names]\n",
    "        if self.dtype:\n",
    "            return X_selected.astype(self.dtype).values\n",
    "        return X_selected.values\n",
    "\n",
    "# Class, for use in pipelines, to binarize nominal-valued features (while avoiding the dummy variabe trap)\n",
    "# By Derek Bridge, 2017\n",
    "class FeatureBinarizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_values):\n",
    "        self.features_values = features_values\n",
    "        self.num_features = len(features_values)\n",
    "        self.labelencodings = [LabelEncoder().fit(feature_values) for feature_values in features_values]\n",
    "        self.onehotencoder = OneHotEncoder(sparse=False,\n",
    "            n_values=[len(feature_values) for feature_values in features_values])\n",
    "        self.last_indexes = np.cumsum([len(feature_values) - 1 for feature_values in self.features_values])\n",
    "    def fit(self, X, y=None):\n",
    "        for i in range(0, self.num_features):\n",
    "            X[:, i] = self.labelencodings[i].transform(X[:, i])\n",
    "        return self.onehotencoder.fit(X)\n",
    "    def transform(self, X, y=None):\n",
    "        for i in range(0, self.num_features):\n",
    "            X[:, i] = self.labelencodings[i].transform(X[:, i])\n",
    "        onehotencoded = self.onehotencoder.transform(X)\n",
    "        return np.delete(onehotencoded, self.last_indexes, axis=1)\n",
    "    def fit_transform(self, X, y=None):\n",
    "        onehotencoded = self.fit(X).transform(X)\n",
    "        return np.delete(onehotencoded, self.last_indexes, axis=1)\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"features_values\" : self.features_values}\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            self.setattr(parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Data types</h1>\n",
    "<ul>\n",
    "    <li>Structured data:\n",
    "        <ul>\n",
    "            <li>\n",
    "                <b>Numeric-valued</b>: either real- or integer-valued, such as floor area or number of bedrooms\n",
    "            </li>\n",
    "            <li>\n",
    "                <b>Nominal-valued</b>: where there is a finite set of possible values. Often these values are strings\n",
    "                <ul>\n",
    "                    <li>For example, dwelling type ($\\mathit{type}$) is a nominal-valued feature whose values are \"Apartment\", \n",
    "                        \"Detached\", \"Semi-detached\" or \"Terraced\". \n",
    "                    </li>\n",
    "                    <li>The special case here is, of course, a binary-valued feature, where\n",
    "                        there are just two values. For example, the type of development ($\\mathit{devment}$) is a nominal-valued feature \n",
    "                        whose values are \"New\" or \"SecondHand\"\n",
    "                    </li>\n",
    "                    <li>Another special case is where there is a finite set of possible values but there is some\n",
    "                        ordering on the values, e.g. the spiciness of a curry can be \"Mild\", \"Medium\", \"Hot\",\n",
    "                        \"Very Hot\" and \"Suicidal\"\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                <b>Set-valued</b>: where the value of a feature is a set, but the members of the set are constrained to\n",
    "                a finite set of nominals. For example, the genre of a movie might be a set-valued feature, e.g. the \n",
    "                value of the genre feature for <i>The Blues Brothers</i> is $\\Set{\\mathit{musical}, \\mathit{comedy},\n",
    "                \\mathit{action}}$.\n",
    "            </li>\n",
    "            <li>&hellip;</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Unstructured: \n",
    "        <ul>\n",
    "            <li>\n",
    "                free-form text\n",
    "            </li>\n",
    "            <li>\n",
    "                media such as images, audio and video\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Data Types in the Cork Propery Dataset</h1>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>$\\mathit{flarea}$</td><td>numeric</td><td>the floor area in square metres</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$\\mathit{type}$</td><td>nominal</td><td>dwelling type: Apartment, Detached, Semi-detached,\n",
    "            Terraced</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$\\mathit{bdrms}$</td><td>numeric</td><td>the number of bedrooms</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$\\mathit{bthrms}$</td><td>numeric</td><td>the number of bathrooms</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$\\mathit{floors}$</td><td>numeric</td><td>the number of floors</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$\\mathit{devment}$</td><td>nominal</td><td>the type of development: New or SecondHand</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$\\mathit{ber}$</td><td>nominal</td>\n",
    "        <td>building energy rating: A1, A2, A3, B1, B2, B3, C1, C2, C3, D1, D2, E1, E2, F, G</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$\\mathit{location}$</td><td>nominal</td><td>the area of Cork, e.g. Douglas, Glanmire, Wilton,...</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"datasets/dataset_corkA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea      float64\n",
       "type         object\n",
       "bdrms         int64\n",
       "bthrms        int64\n",
       "floors        int64\n",
       "devment      object\n",
       "ber          object\n",
       "location     object\n",
       "price         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>type</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>floors</th>\n",
       "      <th>devment</th>\n",
       "      <th>ber</th>\n",
       "      <th>location</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>207.000000</td>\n",
       "      <td>207</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Semi-detached</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SecondHand</td>\n",
       "      <td>G</td>\n",
       "      <td>CityCentre</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>204</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>128.094686</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.434783</td>\n",
       "      <td>2.106280</td>\n",
       "      <td>1.826087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>274.724638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>73.970582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.232390</td>\n",
       "      <td>1.185802</td>\n",
       "      <td>0.379954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>171.756507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>41.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>82.650000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>165.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>225.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>153.650000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>327.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>497.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>995.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            flarea           type       bdrms      bthrms      floors  \\\n",
       "count   207.000000            207  207.000000  207.000000  207.000000   \n",
       "unique         NaN              4         NaN         NaN         NaN   \n",
       "top            NaN  Semi-detached         NaN         NaN         NaN   \n",
       "freq           NaN             65         NaN         NaN         NaN   \n",
       "mean    128.094686            NaN    3.434783    2.106280    1.826087   \n",
       "std      73.970582            NaN    1.232390    1.185802    0.379954   \n",
       "min      41.800000            NaN    1.000000    1.000000    1.000000   \n",
       "25%      82.650000            NaN    3.000000    1.000000    2.000000   \n",
       "50%     106.000000            NaN    3.000000    2.000000    2.000000   \n",
       "75%     153.650000            NaN    4.000000    3.000000    2.000000   \n",
       "max     497.000000            NaN   10.000000   10.000000    2.000000   \n",
       "\n",
       "           devment  ber    location       price  \n",
       "count          207  207         207  207.000000  \n",
       "unique           2   12          36         NaN  \n",
       "top     SecondHand    G  CityCentre         NaN  \n",
       "freq           204   25          40         NaN  \n",
       "mean           NaN  NaN         NaN  274.724638  \n",
       "std            NaN  NaN         NaN  171.756507  \n",
       "min            NaN  NaN         NaN   55.000000  \n",
       "25%            NaN  NaN         NaN  165.000000  \n",
       "50%            NaN  NaN         NaN  225.000000  \n",
       "75%            NaN  NaN         NaN  327.500000  \n",
       "max            NaN  NaN         NaN  995.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>type</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>floors</th>\n",
       "      <th>devment</th>\n",
       "      <th>ber</th>\n",
       "      <th>location</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>497.0</td>\n",
       "      <td>Detached</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>SecondHand</td>\n",
       "      <td>B2</td>\n",
       "      <td>Carrigrohane</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83.6</td>\n",
       "      <td>Detached</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SecondHand</td>\n",
       "      <td>D2</td>\n",
       "      <td>Glanmire</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97.5</td>\n",
       "      <td>Semi-detached</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SecondHand</td>\n",
       "      <td>D1</td>\n",
       "      <td>Glanmire</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flarea           type  bdrms  bthrms  floors     devment ber      location  \\\n",
       "0   497.0       Detached      4       5       2  SecondHand  B2  Carrigrohane   \n",
       "1    83.6       Detached      3       1       1  SecondHand  D2      Glanmire   \n",
       "2    97.5  Semi-detached      3       2       2  SecondHand  D1      Glanmire   \n",
       "\n",
       "   price  \n",
       "0    975  \n",
       "1    195  \n",
       "2    225  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A few of the examples\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Handling Nominal-Valued Features</h1>\n",
    "<ul>\n",
    "    <li>Most AI algorithms work only with numeric-valued features (There are exceptions)</li>\n",
    "    <li>So, we will look at how to convert nominal-valued features to numeric-valued ones</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Binary-valued features</h2>\n",
    "<ul>\n",
    "    <li>The simplest case, obviously, is a binary-valued feature</li>\n",
    "    <li>We encode one value as 0 and the other as 1, e.g. \"SecondHand\" is 0 and \"New\" is 1</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Unordered nominal values</h2>\n",
    "<ul>\n",
    "    <li>Suppose there are more than two values, e.g. Apartment, Detached, Semi-detached or Terraced.</li>\n",
    "    <li>The obvious thing to do is to assign integers to each nominal value, e.g. 0 = Apartment, 1 = Detached, etc.</li>\n",
    "    <li>But often this is not the best encoding\n",
    "        <ul>\n",
    "            <li>Algorithms may assume that the values themselves are meaningful, when they're actually arbitrary\n",
    "                <ul>\n",
    "                    <li>E.g. an algorithm might assume that Apartments (0) are more similar to Detached houses (1)\n",
    "                        than they are to Terraced houses (3)\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Instead, we use <b>one-hot encoding</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>One-Hot Encoding</h3>\n",
    "<ul>\n",
    "    <li>If the original nominal-valued feature has $p$ values, then we use $p$ binary-valued features: \n",
    "        <ul>\n",
    "            <li>In each example, exactly one of them is set to 1 and the rest are zero</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>For example, there are four types of dwelling, so we have four binary-valued features:\n",
    "        <ul>\n",
    "            <li>The first is set to 1 if and only if the type of dwelling is Apartment</li>\n",
    "            <li>The second is set to 1 if and only if the house is Detached</li>\n",
    "            <li>And so on</li>\n",
    "        </ul>\n",
    "        So a detached house will have $\\rv{0, 1, 0, 0}$ as their values\n",
    "    </li>\n",
    "    <li>Some questions:\n",
    "        <ul>\n",
    "            <li>\n",
    "                One-hot encoding replaces one nominal-valued feature that has $p$ values by $p$ binary-valued ones &mdash; in\n",
    "                general, one feature per nominal value. (E.g. $\\mathit{type}$ has four values, so we get four binary features.)\n",
    "                What is the minimum number of binary-valued features we could use? \n",
    "            </li>\n",
    "            <li>\n",
    "                Why don't we use the minimum?\n",
    "            </li>\n",
    "            <li>\n",
    "                Although we get $p$ binary features, we only need $p - 1$. How come? (Advanced note: Look up the\n",
    "                <i>dummy variable trap</i> to see why this might even be preferable)\n",
    "            </li>\n",
    "            <li>\n",
    "                How might one encode a set-valued feature (such as the movie genre example\n",
    "                above)?\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        In practice, it is not uncommon to be given a dataset where a nominal-valued feature has already been \n",
    "        encoded numerically, one integer per value. You might be fooled into thinking that the feature is\n",
    "        numeric-valued and overlook the need to use one-hot encoding on it. Watch out for this!\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Ordered nominal values</h2>\n",
    "<ul>\n",
    "    <li>Consider the case now of a feature whose values are nominal but where there <em>is</em> an ordering\n",
    "        <ul>\n",
    "            <li>E.g. the $\\mathit{ber}$ feature in the housing dataset is like this</li>\n",
    "            <li>In this case, G &lt; F &lt; E2 &lt; E1 &lt; D1 ... &lt; A1</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Some people would use the phrase 'ordinal-valued' to refer to nominal values that have an ordering\n",
    "    </li>\n",
    "    <li>\n",
    "        You might be tempted to use a straightforward numeric encoding\n",
    "        <ul>\n",
    "            <li>E.g. 0 = G, 1 = F, 2 = E2, 3 = E1, and so on</li>\n",
    "            <li>This encoding preserves the ordering, e.g. that E2 &lt; E1 because 2 &lt; 3</li>\n",
    "            <li>But again this is probably not the best encoding\n",
    "                <ul>\n",
    "                    <li>The original feature had an ordering on its values but no notion of distance</li>\n",
    "                    <li>E.g. G &lt; F but you cannot say by <em> how much</em> G is less than F</li>\n",
    "                    <li>In the new feature, we have introduced a notion of distance: G is worse than F by 1, and it is \n",
    "                        2 worse than E2\n",
    "                    </li>\n",
    "                    <li>So this encoding has <em>added</em> 'information' that was not present in the original\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        So what should we do?\n",
    "        <ul>\n",
    "            <li>\n",
    "                We could use one-hot encoding: fifteen binary-valued features. But what are the weaknesses of this?\n",
    "            </li>\n",
    "            <li>\n",
    "                Another option is to use binary-valued features that represent inequalities\n",
    "                <ul>\n",
    "                    <li>E.g. one feature is set to 1 if you have a BER of at least G</li>\n",
    "                    <li>Another is additionally set to 1 if you have attained at least F</li>\n",
    "                    <li>And so on</li>\n",
    "                </ul>\n",
    "                &mdash; still fifteen binary-valued features, but no longer\n",
    "                mutually exclusive\n",
    "                <ul>\n",
    "                    <li>E.g. a BER of E2 is converted to the following fifteen binary-valued\n",
    "                        features: $\\rv{1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}$\n",
    "                    </li>\n",
    "                    <li>E.g. a BER of E1 is converted to\n",
    "                        $\\rv{1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}$\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "        But, since scikit-learn doesn't offer this somewhat sophisticated encoding, and assuming we don't write our own, we will have\n",
    "        to use one-hot encoding\n",
    "    </li>\n",
    "    <li>Again watch out for cases where some well-intentioned person has already encoded this kind of feature\n",
    "        but using a naive numeric encoding\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The curse of dimensionality, again</h2>\n",
    "<ul>\n",
    "    <li>One-hot encoding increases the number of features, sometimes quite a lot</li>\n",
    "    <li>We may need to use dimensionality reduction (although most people don't bother!)\n",
    "        <ul>\n",
    "            <li>Don't use PCA, which is for numeric-valued features</li>\n",
    "            <li>Use, e.g., Canonical Correspondence Analysis (CCA)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Handling Nominal Values in scikit-learn</h1>\n",
    "<ul>\n",
    "    <li>We will add extra steps into our pipeline to convert nominal-values features into numeric ones\n",
    "        <ul>\n",
    "            <li>scikit-learn has some classes for doing this but they do not play nicely with pipelines, so \n",
    "                we wll use my <code>FeatureBinarizer</code> (given\n",
    "                earlier) instead\n",
    "            </li>\n",
    "            <li>(Advanced: <code>FeatureBinarizer</code> avoids the dummy variable trap and uses just $p-1$ binary\n",
    "                features)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But, we now need two pipelines:\n",
    "        <ul>\n",
    "            <li>One takes all the numeric-valued features and, e.g., scales them</li>\n",
    "            <li>The other takes the numeric-valued features and their legal values and binarizes them\n",
    "            </li>\n",
    "        </ul>\n",
    "        You then join the pipelines using <code>FeatureUnion</code>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features we want to select\n",
    "numeric_features = [\"flarea\", \"bdrms\", \"bthrms\", \"floors\"]\n",
    "nominal_features = [\"type\", \"devment\", \"ber\", \"location\"]\n",
    "\n",
    "# Create the pipelines\n",
    "numeric_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector(numeric_features)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "nominal_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector(nominal_features)), \n",
    "        (\"binarizer\", FeatureBinarizer([df[feature].unique() for feature in nominal_features]))])\n",
    "\n",
    "pipeline = Pipeline([(\"union\", FeatureUnion([(\"numeric_pipeline\", numeric_pipeline), \n",
    "                                             (\"nominal_pipeline\", nominal_pipeline)]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "pipeline.fit(df)\n",
    "X = pipeline.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.99927973  0.45974713  2.4462228   0.45883147  0.          1.          0.\n",
      "   1.          0.          1.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          1.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [-0.6029769  -0.35365164 -0.93520037 -2.17944947  0.          1.          0.\n",
      "   1.          0.          0.          0.          0.          0.          0.\n",
      "   0.          1.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   1.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [-0.41460881 -0.35365164 -0.08984458  0.45883147  0.          0.          1.\n",
      "   1.          0.          0.          0.          0.          0.          0.\n",
      "   1.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   1.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at a few rows in X - to show you that we now have a 2D numpy array\n",
    "print(X[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea              134.7\n",
       "type        Semi-detached\n",
       "bdrms                   3\n",
       "bthrms                  2\n",
       "floors                  2\n",
       "devment        SecondHand\n",
       "ber                    D1\n",
       "location         Glasheen\n",
       "price                 245\n",
       "Name: 127, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So which house is most similar to yours, now that we are using all the features?\n",
    "\n",
    "def euc(x, xprime):\n",
    "    return np.sqrt(np.sum((x - xprime)**2))\n",
    "\n",
    "# Don't try to understand or copy this code - it's a hack that you won't need\n",
    "your_house_df = pd.DataFrame([{\"flarea\":114.0, \"type\":\"Semi-detached\", \"bdrms\":3, \"bthrms\":2, \"floors\":2,  \n",
    "                               \"devment\":\"SecondHand\", \"ber\":\"B2\", \"location\":\"Glasheen\"}])\n",
    "your_house_scaled = pipeline.transform(your_house_df)[0]\n",
    "\n",
    "df.ix[np.argmin([euc(your_house_scaled, x) for x in X])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Actually, there is a question of whether Euclidean distance is the best distance measure to use on nominal-valued\n",
    "        features and on mixtures of numeric-valued features and nominal-valued features\n",
    "    </li>\n",
    "    <li>But, in this introductory module, we will use it!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Free-Form Text</h1>\n",
    "<ul>\n",
    "    <li>Suppose the objects in your dataset are <b>documents</b>, rather than houses\n",
    "        <ul>\n",
    "            <li>E.g. web pages, tweets, blog posts, emails, posts to Internet forums and chatrooms, &hellip;</li>\n",
    "            <li>They might have a little structure to them (headings and so on), but they are primarily\n",
    "                <b>free-form text</b>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Many AI algorithms can only handle vectors of numbers. So one way to apply AI techniques to \n",
    "        a dataset of documents is to convert the raw text in the documents into vectors of numbers\n",
    "    </li>\n",
    "    <li>Our treatment of this will be brief and high-level, since many of you are studying\n",
    "        <i>CS4611 Information Retrieval</i>, where this is covered in depth\n",
    "    </li>\n",
    "    <li>Furthermore, we'll use scikit-learn although its facilities for handling text are quite limited. \n",
    "        If you really want to do AI with text, consider a more powerful library such as <i>NLTK</i>\n",
    "        (<a href=\"http://www.nltk.org/\">http://www.nltk.org/</a>) or the <i>Stanford Natural Language\n",
    "        Processing Toolkit</i> \n",
    "        (<a href=\"https://nlp.stanford.edu/software/\">https://nlp.stanford.edu/software/</a>) \n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Running Example</h2>\n",
    "<p>\n",
    "    Suppose our dataset contains just these three documents:\n",
    "</p>\n",
    "<table>\n",
    "    <tr><th>Tweet 0</th><th>Tweet 1</th><th>Tweet 2</th></tr>\n",
    "    <tr>\n",
    "        <td>No one is born hating another person because of the color of his skin or his background \n",
    "            or his religion.\n",
    "        </td>\n",
    "        <td>People must learn to hate, and if they can learn to hate, they can be taught to love.</td>\n",
    "        <td>For love comes more naturally to the human heart than its opposite.</td>\n",
    "     </tr>\n",
    "     <caption style=\"caption-side: bottom; text-align: center\">\n",
    "         Three tweets from Barack Obama, quoting Nelson Mandela\n",
    "     </caption>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Bag-of-words representation</h2>\n",
    "<ul>\n",
    "    <li><b>Tokenize</b> each document\n",
    "        <ul>\n",
    "            <li>In our simple treatment, the tokens are just the words, ignoring punctuation and making everything\n",
    "                lowercase\n",
    "            </li>\n",
    "            <li>In reality, this is surprisingly complicated, e.g. is \"don't\" one token or two, e.g. maybe\n",
    "                pairs of consecutive words (so-called 'bigrams') could also be tokens (\"no one\", \"one is\",\n",
    "                \"is born\"); and so on\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Optionally, discard <b>stop-words</b>: common words such as \"a\", \"the\", \"in\", \"on\", \"is, \"are\",&hellip;\n",
    "        <ul>\n",
    "            <li>Sometimes discarding them helps, or does no harm, e.g. spam detection</li>\n",
    "            <li>Other times, you lose too much, e.g. web search engines (\"To be, or not to be\")</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'yet', 'though', 'on', 'until', 'somewhere', 'out', 'whereby', 'forty', 'hasnt', 'hereupon', 'latter', 'yours', 'is', 'whither', 'might', 'serious', 'nine', 'whether', 'are', 'five', 'such', 'what', 'am', 'therein', 'thereupon', 'while', 'somehow', 'couldnt', 'where', 'thereby', 'never', 'ours', 'you', 'further', 'within', 'twenty', 'last', 'some', 'empty', 'everywhere', 'something', 'thick', 'onto', 'those', 'about', 'nor', 'wherein', 'there', 'whatever', 'be', 'become', 'which', 'himself', 'nowhere', 'yourself', 'via', 'amoungst', 'ever', 'often', 'his', 'against', 'always', 'will', 'everyone', 'namely', 'three', 'again', 'anyhow', 'whenever', 'from', 'per', 'rather', 'co', 'many', 'had', 'third', 'since', 'anyway', 'this', 'whereafter', 'either', 'and', 'without', 'by', 'even', 'eleven', 'it', 'mine', 'up', 'moreover', 'noone', 'perhaps', 'became', 'fifty', 'any', 'please', 'too', 'could', 'detail', 'thin', 'fire', 'un', 'all', 'show', 'hereby', 'indeed', 'can', 'find', 'not', 'whom', 'meanwhile', 'over', 'hundred', 'four', 'see', 'now', 'sometime', 'enough', 'although', 'side', 'sixty', 'back', 'being', 'along', 'under', 'he', 'would', 'anyone', 'part', 'made', 'seemed', 'the', 'only', 'across', 'below', 'of', 'sometimes', 'whole', 'bill', 'someone', 'hereafter', 'same', 'if', 'cry', 'therefore', 'ten', 'elsewhere', 'toward', 'seeming', 'otherwise', 'with', 'few', 'another', 'cannot', 'in', 'full', 'one', 'top', 'whence', 'ie', 'describe', 'move', 'my', 'cant', 'herein', 'ltd', 'so', 'nevertheless', 'behind', 'themselves', 'do', 'several', 'thus', 'alone', 'call', 'down', 'i', 'our', 'anywhere', 'less', 'than', 'towards', 'may', 'whereupon', 'your', 'fifteen', 'or', 'still', 'was', 'well', 'inc', 'becoming', 'at', 'me', 'becomes', 'for', 'mostly', 'should', 'nothing', 'off', 'eg', 'during', 'that', 'we', 'six', 'these', 'but', 'seems', 'were', 'already', 'thereafter', 'hence', 'once', 'because', 'give', 'interest', 'him', 'none', 'everything', 'mill', 'put', 'get', 'them', 'whose', 're', 'anything', 'front', 'others', 'into', 'she', 'name', 'very', 'found', 'beside', 'con', 'after', 'bottom', 'both', 'here', 'among', 'how', 'most', 'hers', 'go', 'above', 'through', 'system', 'thence', 'afterwards', 'between', 'former', 'else', 'next', 'they', 'together', 'her', 'twelve', 'as', 'has', 'amongst', 'due', 'sincere', 'ourselves', 'beforehand', 'first', 'also', 'before', 'more', 'wherever', 'who', 'yourselves', 'its', 'almost', 'whereas', 'upon', 'herself', 'however', 'why', 'no', 'must', 'a', 'thru', 'etc', 'de', 'itself', 'formerly', 'keep', 'each', 'an', 'other', 'much', 'myself', 'beyond', 'been', 'seem', 'own', 'throughout', 'besides', 'done', 'neither', 'when', 'their', 'every', 'around', 'nobody', 'to', 'then', 'except', 'least', 'us', 'amount', 'eight', 'fill', 'latterly', 'whoever', 'have', 'take', 'two'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    " \n",
    "print(stop_words.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Optionally, apply <b>stemming</b> or <b>lemmatization</b> to the words\n",
    "        <ul>\n",
    "            <li>E.g. \"hating\" is replaced by \"hate\", \"comes\" is replaced by \"come\"</li>\n",
    "            <li>scikit-learn doesn't have a stemmer, but does make it easy to call one, if you get one from another library, \n",
    "                e.g. NLTK\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Count Vectorize</b>: each document becomes a vector, each token becomes a feature, feature-values are\n",
    "        <em>frequencies</em> (how many times that token appears in that document)<br />\n",
    "        (In <i>CS4611</i>, features are probably referred to as 'terms')\n",
    "    </li>\n",
    "    <li>Optionally, <b>TD-IDF Vectorize</b>: replace the frequencies by <b>tf-idf</b> scores\n",
    "        <ul>\n",
    "            <li>tf-idf scores penalise words that recur across multiple documents</li>\n",
    "            <li>E.g. in emails, word such as \"hi\", \"best\", \"regards\", &hellip;</li>\n",
    "            <li>For the formulae, see <i>CS4611</i>\n",
    "                <ul>\n",
    "                    <li>variants might: scale frequencies to avoid biases towards long documents (not scikit-learn);\n",
    "                        logarithmically scale frequencies (not default in scikit-learn);\n",
    "                        add 1 to part of the formula to avoid division-by-zero (default in scikit-learn);\n",
    "                        normalize the results (e.g. by default, scikit-learn divides by the L2-norm)\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Running Example</h2>\n",
    "<ul>\n",
    "    <li>After discarding stop-words:\n",
    "<table>\n",
    "    <tr><th>Tweet 0</th><th>Tweet 1</th><th>Tweet 2</th></tr>\n",
    "    <tr>\n",
    "        <td>born hating person color skin background religion\n",
    "        </td>\n",
    "        <td>people learn hate learn hate taught love</td>\n",
    "        <td>love comes naturally human heart opposite</td>\n",
    "     </tr>\n",
    "</table>\n",
    "    </li>\n",
    "    <li>After count vectorization:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>background</th>\n",
    "        <th>born</th>\n",
    "        <th>color</th>\n",
    "        <th>comes</th>\n",
    "        <th>hate</th>\n",
    "        <th>hating</th>\n",
    "        <th>heart</th>\n",
    "        <th>human</th>\n",
    "        <th>learn</th>\n",
    "        <th>love</th>\n",
    "        <th>naturally</th>\n",
    "        <th>opposite</th>\n",
    "        <th>people</th>\n",
    "        <th>person</th>\n",
    "        <th>religion</th>\n",
    "        <th>skin</th>\n",
    "        <th>taught</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Tweet 0:</th>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Tweet 1:</th>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>2</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>2</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Tweet 2:</th>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "</table>\n",
    "    </li>\n",
    "    <li>After tf-idf vectorization:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>background</th>\n",
    "        <th>born</th>\n",
    "        <th>color</th>\n",
    "        <th>comes</th>\n",
    "        <th>hate</th>\n",
    "        <th>hating</th>\n",
    "        <th>heart</th>\n",
    "        <th>human</th>\n",
    "        <th>learn</th>\n",
    "        <th>love</th>\n",
    "        <th>naturally</th>\n",
    "        <th>opposite</th>\n",
    "        <th>people</th>\n",
    "        <th>person</th>\n",
    "        <th>religion</th>\n",
    "        <th>skin</th>\n",
    "        <th>taught</th>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <th>Tweet 0:</th>\n",
    "        <td>0.38</td>\n",
    "        <td>0.38</td>\n",
    "        <td>0.38</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.38</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.38</td>\n",
    "        <td>0.38</td>\n",
    "        <td>0.38</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Tweet 1:</th>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.61</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.61</td>\n",
    "        <td>0.23</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.31</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.31</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Tweet 2:</th>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.42</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.42</td>\n",
    "        <td>0.42</td>\n",
    "        <td>0</td>\n",
    "        <td>0.32</td>\n",
    "        <td>0.42</td>\n",
    "        <td>0.42</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "</table>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The dimension of these vectors</h2>\n",
    "<ul>\n",
    "    <li>Sparsity:\n",
    "        <ul>\n",
    "            <li>Here we had $n = 17$ features (columns). How many will there be in general?</li>\n",
    "            <li>Most of the feature-values are zero. Why?</li>\n",
    "            <li>We say that the matrix is <b>sparse</b></li>\n",
    "            <li>It would be wasteful to store it using very long arrays. We need a data structure that only \n",
    "                stores the non-zero elements: <b>sparse matrices</b><br />\n",
    "                (Don't worry: scikit-learn takes care of this 'behind the scenes')\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The curse of dimensionality, yet again:\n",
    "        <ul>\n",
    "            <li>Reduce the number of features by\n",
    "                <ul>\n",
    "                    <li>discarding tokens that appear in too few documents (<code>min_df</code> in scikit-learn)\n",
    "                    </li>\n",
    "                    <li>discarding tokens that appear in too many documents (<code>max_df</code>)</li>\n",
    "                    <li>keeping only the most frequent tokens (<code>max_features</code>)</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Use dimensionality reduction:\n",
    "                <ul>\n",
    "                    <li>E.g. singular value decomposition (SVD) is suitable for bag-of-words, rather than PCA</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Observation about bag-of-words representations</h2>\n",
    "<ul>\n",
    "    <li>This representation is good for many applications in AI but it does have drawbacks too:\n",
    "        <ul>\n",
    "            <li>It loses all the information that English conveys through the order of words in sentences\n",
    "                <ul>\n",
    "                    <li>E.g. \"People learn to hate\" and \"People hate to learn\" have very different meanings but\n",
    "                        end up with the same bag-of-words representation\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>It loses the information that English conveys using its stop-words, most notably negation\n",
    "                <ul>\n",
    "                    <li>E.g. \"They hate religion\" and \"I do not hate religion\" will have the same bag-of-words\n",
    "                        representation\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This may not matter for some applications (e.g. spam detection) but will matter for\n",
    "        others (e.g. machine translation), for which you need a different representation\n",
    "    </li>\n",
    "    <li>What other weaknesses does it have?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bag-of-words representation in scikit-learn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"No one is born hating another person because of the color of his skin or his background or his religion.\",\n",
    "    \"People must learn to hate, and if they can learn to hate, they can be taught to love.\",\n",
    "    \"For love comes more naturally to the human heart than its opposite.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>In the example below, we put a <code>CountVectorizer</code> into a pipeline</li>\n",
    "    <li>It does tokenization\n",
    "        <ul>\n",
    "            <li>By default, it converts to lowercase, it treats punctuation as spaces, and it treats two or more\n",
    "                consecutive characters as a word. Each word becomes a token (feature)\n",
    "            </li>\n",
    "        </ul>\n",
    "        <li>The example below discards stop-words using the list we saw earlier</li>\n",
    "        <li>It also, by default, discards any word that appears in every document</li>\n",
    "        <li>It does not do stemming or lemmatization but there are ways of incorporating a stemmer from, e.g., NLTK</li>\n",
    "        <li>Finally, it vectorizes, producing sparse matrices of word fequencies. \n",
    "            (There is an option to produce a binary representation, instead of frequencies)\n",
    "        </li>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "text_pipeline = Pipeline([\n",
    "        (\"vectorizer\", CountVectorizer(stop_words='english'))\n",
    "    ])\n",
    "\n",
    "# Run the pipeline\n",
    "text_pipeline.fit(tweets)\n",
    "X = text_pipeline.transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['background',\n",
       " 'born',\n",
       " 'color',\n",
       " 'comes',\n",
       " 'hate',\n",
       " 'hating',\n",
       " 'heart',\n",
       " 'human',\n",
       " 'learn',\n",
       " 'love',\n",
       " 'naturally',\n",
       " 'opposite',\n",
       " 'people',\n",
       " 'person',\n",
       " 'religion',\n",
       " 'skin',\n",
       " 'taught']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the features\n",
    "text_pipeline.named_steps[\"vectorizer\"].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 8)\t2\n",
      "  (1, 9)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 16)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "# We can look at the sparse array. The first number identifies the tweet (0, 1 or 2), the second is which feature, and\n",
    "# the last is the frequency\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize a new document\n",
    "new_document = \"Unsurprisingly, people hate to learn that their religion loves to hate.\"\n",
    "\n",
    "new_document_as_vector = text_pipeline.transform([new_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t2\n",
      "  (0, 8)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 14)\t1\n"
     ]
    }
   ],
   "source": [
    "# Notice how it ignores words that weren't in the original tweets, such as \"unsurprisingly\" and \"loves\"\n",
    "\n",
    "print(new_document_as_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>In the example below, we put a <code>TfidfVectorizer</code> into a pipeline instead</li>\n",
    "    <li>By default, it normalizes the values using the L2 norm (see CS46111)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "text_pipeline = Pipeline([\n",
    "        (\"vectorizer\", TfidfVectorizer(stop_words='english'))\n",
    "    ])\n",
    "\n",
    "# Run the pipeline\n",
    "text_pipeline.fit(tweets)\n",
    "X = text_pipeline.transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15)\t0.377964473009\n",
      "  (0, 14)\t0.377964473009\n",
      "  (0, 13)\t0.377964473009\n",
      "  (0, 5)\t0.377964473009\n",
      "  (0, 2)\t0.377964473009\n",
      "  (0, 1)\t0.377964473009\n",
      "  (0, 0)\t0.377964473009\n",
      "  (1, 16)\t0.307460988215\n",
      "  (1, 12)\t0.307460988215\n",
      "  (1, 9)\t0.233832006484\n",
      "  (1, 8)\t0.614921976431\n",
      "  (1, 4)\t0.614921976431\n",
      "  (2, 11)\t0.423394483412\n",
      "  (2, 10)\t0.423394483412\n",
      "  (2, 9)\t0.322002417819\n",
      "  (2, 7)\t0.423394483412\n",
      "  (2, 6)\t0.423394483412\n",
      "  (2, 3)\t0.423394483412\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize a new document\n",
    "new_document = \"Unsurprisingly, people hate to learn that their religion loves to hate.\"\n",
    "\n",
    "new_document_as_vector = text_pipeline.transform([new_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14)\t0.377964473009\n",
      "  (0, 12)\t0.377964473009\n",
      "  (0, 8)\t0.377964473009\n",
      "  (0, 4)\t0.755928946018\n"
     ]
    }
   ],
   "source": [
    "# Notice how it ignores words that weren't in the original tweets, such as \"unsurprisingly\" and \"loves\"\n",
    "\n",
    "print(new_document_as_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Similarity &amp; distance for bag-of-words representation</h2>\n",
    "<ul>\n",
    "    <li>For details and formulae, see CS4611</li>\n",
    "    <li>Euclidean distance is not suitable</li>\n",
    "    <li>Very common is <b>cosine similarity</b>, which gives values in $[0, 1]$, where 1 means 'identical'</li>\n",
    "    <li>To get <b>cosine distance</b>, we can subtract from 1, so now 1 means 'completely different'</li>\n",
    "    <li>The exact formulae differ depending on what is assumed about normalization\n",
    "        <ul>\n",
    "            <li>If we assume the vectors have been normalized, then simpler formula</li>\n",
    "            <li>If not, then the formula is more complicated</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Similarity &amp; distance for bag-of-words representation in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>The code below assumes that the vectors have already been normalized, e.g. produced\n",
    "        by <code>TfidfVectorizer</code>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(x, xprime):\n",
    "    # Assumes x and  xprime are already normalized\n",
    "    # Converts from sparse matrices because np.dot does not work on them\n",
    "    return 1 - x.toarray().dot(xprime.toarray().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People must learn to hate, and if they can learn to hate, they can be taught to love.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So which of Barack Obama's tweets is most similar to our new document?\n",
    "tweets[np.argmin([cosine(new_document_as_vector, x) for x in X])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
