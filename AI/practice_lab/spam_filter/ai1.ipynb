{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b> Build spam filters/classifiers and evaluate them. </b> </p>\n",
    "<p> <b> Input: </b> spam and ham, containing spam or non-spam emails respectively, these are text files. </p>\n",
    "<p> <b> Output: </b> a Jupyter notebook named ai1.ipynb containing:\n",
    "classifiers and their comparison/evaluation, dataset exploration, dataset preprocessing, \n",
    "and most importantly markup cells explaing the whats, the hows, and the whys of the problem and the solution. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Class, for use in pipelines, to select certain columns from a DataFrame and convert to a numpy array\n",
    "# From A. Geron: Hands-On Machine Learning with Scikit-Learn & TensorFlow, O'Reilly, 2017\n",
    "# Modified by Derek Bridge to allow for casting in the same ways as pandas.DataFrame.astype\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype=None):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_selected = X[self.attribute_names]\n",
    "        if self.dtype:\n",
    "            return X_selected.astype(self.dtype).values\n",
    "        return X_selected.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Extracting the archives, we can see that there are two kinds emails (spam and ham, obviously) in different folders. </p>\n",
    "<p> So we can identify the label of an example by looking at the directory it is stored in. </p>\n",
    "<p> Opening some of the emails, we can observe that each one of them is starting with some metadata about the email itself, and the content of the email is following the metadata. Some HTML tags are present. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some functions to read the dataset from the files provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reads all files from the directory specified, and their content is returned as\n",
    "# a pandas Series of strings. \n",
    "def read_files_from_dir(directory):\n",
    "    files_contents = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path) as f:\n",
    "            files_contents.append(f.read())\n",
    "    return pd.Series(files_contents)\n",
    "\n",
    "# Converts a series to a dataframe and adds for each of the elements a \n",
    "# constant numeric label, specified in the parameter label. \n",
    "def to_pd_DF_with_label(ser, label):\n",
    "    df = pd.DataFrame()\n",
    "    df['text'] = ser\n",
    "    df['label'] = pd.Series(np.ones(len(df), dtype=np.int64) * label, index=df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Now we will use the functions we defined, to actually read in the dataset. </p>\n",
    "<p> These lines of code assume that the spam and ham archives\n",
    "have been extracted to directories spam and ham. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read hams with label 0, since they are the negative class\n",
    "hams = to_pd_DF_with_label(read_files_from_dir('ham'), 0)\n",
    "# read spams with label 1, since they are the positive class\n",
    "spams = to_pd_DF_with_label(read_files_from_dir('spam'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1650, 2)\n",
      "(1248, 2)\n"
     ]
    }
   ],
   "source": [
    "# check if we succeeded in reading in the dataset.\n",
    "print(hams.shape)\n",
    "print(spams.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Now that we have two separate dataframes, we should append one to the other,\n",
    "to have all data data in a single dataframe. </p>\n",
    "<p> After the append, we know that all hams are before all the spams,\n",
    "so we should shuffle the dataset to avoid problems with k-fold in the future. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2898, 2)\n"
     ]
    }
   ],
   "source": [
    "emails = hams.append(spams, ignore_index=True)\n",
    "emails = emails.take(np.random.permutation(len(emails)))\n",
    "emails.reset_index(drop=True, inplace=True)\n",
    "print(emails.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Opening the email files, we can see that the first lines of all the emails \n",
    "are data about the email itself (metadata).\n",
    "Since we do not want to conduct metadata analysis of the email,\n",
    "we can delete this metadata, leaving us with the title and the body of the email. </p>\n",
    "<p> In order to strip the metadata we have to identify it.\n",
    "After opening a few files, I noticed a pattern:\n",
    "the metadata is delimited by an empty line in the files. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just a friendly regex to delete everyting before a double newline (\\n\\n)\n",
    "emails['stripped_metadata'] = emails['text'].str.replace(r'(.*?)\\n\\n', '', flags=re.MULTILINE | re.DOTALL, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got rid of the metadata, the next thing I think to be unnecessary\n",
    "is the data found between HTML tags, so we could remove those too,\n",
    "in order to remain with only the plain text of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# another regex to delete everyting between '<' and '>'\n",
    "emails['just_text'] = emails['stripped_metadata'].str.replace(r\"<(.*?)>\", '', flags=re.MULTILINE | re.DOTALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run some tests later, I will strip the HTLM tags also,\n",
    "while leaving the metadata, so we can comapare these two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emails['stripped_html'] = emails['text'].str.replace(r\"<(.*?)>\", '', flags=re.MULTILINE | re.DOTALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All types of preprocessing are stored in this list of tuples,\n",
    "where the first element of the tuple represents name of the preprocessing type\n",
    "and the second element of the tuple represents the corresponding column from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessings = [('raw text', 'text'), \n",
    "                  ('stripped metadata', 'stripped_metadata'),\n",
    "                  ('stripped HTML tags', 'stripped_html'),\n",
    "                  ('stripped metadata and HTML', 'just_text'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> In the following section, we will build the pipelines,\n",
    "which will be responsible for doing the vectorization of the emails and their classification. </p>\n",
    "<p> For each pipeline you can find a short comment describing the decisions I took while building the pipeline. </p>\n",
    "<p> Please note: I did not present here all the possibilities that I tried,\n",
    "since the results are really similiar, and there is no need to replicate so much of the work. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b> SVD scaler </b> </p>\n",
    "SVD stands for Singular Value Decomposition, perform dimensonality reduction. Especially useful on the bag of words representation, since this implementation supports sparse matrices as input.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some paramters to CountVectorizer: minimum document frequency should be 0.01 and max 0.3\n",
    "# these parameters are included in order to discard number which appear too often, or\n",
    "# too rarely, this way avoiding word which apper once or so.\n",
    "\n",
    "# SVD -- Singular Value Decomposition for dimensinality reduction.\n",
    "# maximum number of features will be 100, and the number of iterations 7\n",
    "\n",
    "# default LogisticRegression\n",
    "count_vect_eng_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english', min_df=0.01, max_df=0.3)),\n",
    "    ('dim_reduc', TruncatedSVD(n_components=100, n_iter=7)),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# some paramters to tf-idf vectorizer: minimum document frequency should be 0.01 and max 0.5\n",
    "\n",
    "# default LogisticRegression\n",
    "tfidf_eng_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', min_df=0.01, max_df=0.5)),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# For SGD I am using the hinge loss function,\n",
    "# if the log function would be used, we would get Logistic Regression\n",
    "\n",
    "#  CountVectorizer -- limit the number of features to 10000\n",
    "# TruncatedSVD -- creates 500 new features in 10 iterations\n",
    "# SGDClassifier\n",
    "count_vect_pipeline_sgd = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_features=10000)),\n",
    "    ('dim_reduc', TruncatedSVD(n_components=500, n_iter=10)),\n",
    "    ('classifier', SGDClassifier(max_iter=1000, loss='hinge')),\n",
    "])\n",
    "\n",
    "# TfidfVectorizer + SGDClassifier, using hinge loss function\n",
    "\n",
    "tfidf_pipeline_sgd = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000)),\n",
    "    ('classifier', SGDClassifier(max_iter=1000, loss='hinge')),\n",
    "])\n",
    "\n",
    "# just a dummy pipeline, which will always predict the mode.\n",
    "dummy_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(['label'])),\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent')),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same idea is used as when constructing the preprocessings list: \n",
    "list of tuples, first name, second pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [('Count vectorizer with English stop words', count_vect_eng_pipeline), \n",
    "             ('tf-idf vectorizer with English stop words', tfidf_eng_pipeline), \n",
    "             ('Count vectorizer + SGD', count_vect_pipeline_sgd),\n",
    "             ('tf-idf vectorizer + SGD', tfidf_pipeline_sgd),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start estimating the accuracy of the pipelines, \n",
    "we should talk about the accuracy measures we use in order to evaluate the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> I chose stratified k-fold as my accuracy measure, with k=10,\n",
    "because we have a lot of examples, so it will perform better than Holdout.\n",
    "In each fold we will have more than 289 example. \n",
    "Having so many examples in each fold ensures that the measurement is \n",
    "statistically significant (we should have at least 30 examples in each fold). </p>\n",
    "\n",
    "<p>Stratified k-fold is better than holdout (in this case), due to the fact that,\n",
    "k-fold will perform the stratification and testing multiple times, \n",
    "so the chances of getting 'unlucky' will be less, compared to using simple holdout. </p>\n",
    "\n",
    "<p> When using k-fold, it is important to shuffle the dataset, \n",
    "since if the dataset is sorted, in each fold a particular type \n",
    "of examples may be included, which will result in a skewed result. </p>\n",
    "\n",
    "<p> The shuffling of the dataset has been done previously. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the labels of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the labels\n",
    "y = emails['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how the dummy classifier performs, which will predict the most frequent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56935926500417611"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(dummy_pipeline, emails, y, scoring='accuracy', cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the accuracy of a classifier, with 10-fold cross validation, and stripped metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94996420474883669"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(count_vect_eng_pipeline, emails['stripped_metadata'], y, scoring='accuracy', cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1597,   53],\n",
       "       [  86, 1162]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = cross_val_predict(count_vect_eng_pipeline, emails['stripped_metadata'], y, cv=10)\n",
    "confusion_matrix(y, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The confusion matrix will be handy to compare which classifiers are generating the most False Positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual testing is nice and all, but we have too many possibilities to check we should automate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the estimation for all pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> In order to test the above presented pipelines and with each of the possible preprocessing steps, I wrote two simple fors, which will check all possible combinations of these two. </p>\n",
    "\n",
    "<p> In order to provide more comprehensive analysis, I will use more than one performance measurement. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Precison gives the ratio of correctly classified negative examples.\n",
    "Recall is the ability of the classifier to find all positive examples. </p>\n",
    "\n",
    "<p> These two performance measures are complementary, so there is third one, called f1,\n",
    "which the weighted harmonic mean of the precision and recall. </p>\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall#Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "Count vectorizer with English stop words raw text 0.965490991528\n",
      "Count vectorizer with English stop words stripped metadata 0.952034363441\n",
      "Count vectorizer with English stop words stripped HTML tags 0.962042715666\n",
      "Count vectorizer with English stop words stripped metadata and HTML 0.961344708269\n",
      "tf-idf vectorizer with English stop words raw text 0.954796563656\n",
      "tf-idf vectorizer with English stop words stripped metadata 0.959973750149\n",
      "tf-idf vectorizer with English stop words stripped HTML tags 0.961014198783\n",
      "tf-idf vectorizer with English stop words stripped metadata and HTML 0.967215129459\n",
      "Count vectorizer + SGD raw text 0.962728791314\n",
      "Count vectorizer + SGD stripped metadata 0.956523087937\n",
      "Count vectorizer + SGD stripped HTML tags 0.956865529173\n",
      "Count vectorizer + SGD stripped metadata and HTML 0.954103328958\n",
      "tf-idf vectorizer + SGD raw text 0.982745495764\n",
      "tf-idf vectorizer + SGD stripped metadata 0.981020164658\n",
      "tf-idf vectorizer + SGD stripped HTML tags 0.987230640735\n",
      "tf-idf vectorizer + SGD stripped metadata and HTML 0.979984488724\n",
      "\n",
      "f1\n",
      "Count vectorizer with English stop words raw text 0.958844538668\n",
      "Count vectorizer with English stop words stripped metadata 0.943012871527\n",
      "Count vectorizer with English stop words stripped HTML tags 0.960251422679\n",
      "Count vectorizer with English stop words stripped metadata and HTML 0.954348517174\n",
      "tf-idf vectorizer with English stop words raw text 0.947255711372\n",
      "tf-idf vectorizer with English stop words stripped metadata 0.953718635129\n",
      "tf-idf vectorizer with English stop words stripped HTML tags 0.954232050501\n",
      "tf-idf vectorizer with English stop words stripped metadata and HTML 0.962102649951\n",
      "Count vectorizer + SGD raw text 0.955803866706\n",
      "Count vectorizer + SGD stripped metadata 0.94678102813\n",
      "Count vectorizer + SGD stripped HTML tags 0.952910318554\n",
      "Count vectorizer + SGD stripped metadata and HTML 0.950897432956\n",
      "tf-idf vectorizer + SGD raw text 0.978703218954\n",
      "tf-idf vectorizer + SGD stripped metadata 0.977937796554\n",
      "tf-idf vectorizer + SGD stripped HTML tags 0.98552431622\n",
      "tf-idf vectorizer + SGD stripped metadata and HTML 0.976693745896\n",
      "\n",
      "precision\n",
      "Count vectorizer with English stop words raw text 0.961291133679\n",
      "Count vectorizer with English stop words stripped metadata 0.956177104802\n",
      "Count vectorizer with English stop words stripped HTML tags 0.959326857568\n",
      "Count vectorizer with English stop words stripped metadata and HTML 0.952904666226\n",
      "tf-idf vectorizer with English stop words raw text 0.951693295619\n",
      "tf-idf vectorizer with English stop words stripped metadata 0.948475345582\n",
      "tf-idf vectorizer with English stop words stripped HTML tags 0.964191693314\n",
      "tf-idf vectorizer with English stop words stripped metadata and HTML 0.957157951768\n",
      "Count vectorizer + SGD raw text 0.956581884589\n",
      "Count vectorizer + SGD stripped metadata 0.94389205927\n",
      "Count vectorizer + SGD stripped HTML tags 0.959942845888\n",
      "Count vectorizer + SGD stripped metadata and HTML 0.946936850953\n",
      "tf-idf vectorizer + SGD raw text 0.979198731937\n",
      "tf-idf vectorizer + SGD stripped metadata 0.979305478972\n",
      "tf-idf vectorizer + SGD stripped HTML tags 0.988079532192\n",
      "tf-idf vectorizer + SGD stripped metadata and HTML 0.979242137621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for scor in ['accuracy', 'f1', 'precision']:\n",
    "    print(scor)\n",
    "    for pipeline_name, pipeline in pipelines:\n",
    "        for preproc_name, preproc in preprocessings:\n",
    "            mean = np.mean(cross_val_score(pipeline, emails[preproc], y, scoring=scor, cv=10))\n",
    "            print(pipeline_name, preproc_name, mean)\n",
    "    print();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p> Intrestingly, the tf-idf vectorizer performed generally poorer,\n",
    "than the CountVectorizer when used with LogisticRegression.\n",
    "However using Stochastic Gradient Descent tf-idf will shine. </p>\n",
    "\n",
    "<p> The CountVectorizer seems to lose accuracy \n",
    "as the data is preprocessed, the tf-idf approach is gaining accuracy if the data is preprocessed. </p>\n",
    "\n",
    "<p> Despite the fact that the first classifier, count vectorizer with english stop words, uses only 100 features, it performed quite well. Approxamitely 97% in all performance metrics. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also it looks like discarding the English stopwords has little effects, \n",
    "but the pipelines without the discarding of stopwords are performing slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The best classifier, according to this analysis is tf-idf vectorizer with stripped HTML tags, without english stopwords and using SGD, since it has the highest score on all performance metrics. </p>\n",
    "\n",
    "<p> This test has been run multiple times in order to check if randomness playes a role, but I found that the results are really similar. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices for all of the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer with English stop words raw text\n",
      "[[1605   45]\n",
      " [  60 1188]]\n",
      "Count vectorizer with English stop words stripped metadata\n",
      "[[1591   59]\n",
      " [  80 1168]]\n",
      "Count vectorizer with English stop words stripped HTML tags\n",
      "[[1597   53]\n",
      " [  57 1191]]\n",
      "Count vectorizer with English stop words stripped metadata and HTML\n",
      "[[1595   55]\n",
      " [  58 1190]]\n",
      "tf-idf vectorizer with English stop words raw text\n",
      "[[1590   60]\n",
      " [  71 1177]]\n",
      "tf-idf vectorizer with English stop words stripped metadata\n",
      "[[1585   65]\n",
      " [  51 1197]]\n",
      "tf-idf vectorizer with English stop words stripped HTML tags\n",
      "[[1606   44]\n",
      " [  69 1179]]\n",
      "tf-idf vectorizer with English stop words stripped metadata and HTML\n",
      "[[1596   54]\n",
      " [  41 1207]]\n",
      "Count vectorizer + SGD raw text\n",
      "[[1598   52]\n",
      " [  55 1193]]\n",
      "Count vectorizer + SGD stripped metadata\n",
      "[[1578   72]\n",
      " [  59 1189]]\n",
      "Count vectorizer + SGD stripped HTML tags\n",
      "[[1599   51]\n",
      " [  62 1186]]\n",
      "Count vectorizer + SGD stripped metadata and HTML\n",
      "[[1582   68]\n",
      " [  58 1190]]\n",
      "tf-idf vectorizer + SGD raw text\n",
      "[[1624   26]\n",
      " [  27 1221]]\n",
      "tf-idf vectorizer + SGD stripped metadata\n",
      "[[1624   26]\n",
      " [  29 1219]]\n",
      "tf-idf vectorizer + SGD stripped HTML tags\n",
      "[[1636   14]\n",
      " [  22 1226]]\n",
      "tf-idf vectorizer + SGD stripped metadata and HTML\n",
      "[[1624   26]\n",
      " [  32 1216]]\n"
     ]
    }
   ],
   "source": [
    "for pipeline_name, pipeline in pipelines:\n",
    "    for preproc_name, preproc in preprocessings:\n",
    "        y_predicted = cross_val_predict(pipeline, emails[preproc], y, cv=10)\n",
    "        conf_matrix = confusion_matrix(y, y_predicted)\n",
    "        print(pipeline_name, preproc_name)\n",
    "        print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusion matrices we arrive at the same conclusion as before, the best classifier is tf-idf with SGD and stipped HTML tags, since it makes the least number of False Positives (ham classified as spam)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
