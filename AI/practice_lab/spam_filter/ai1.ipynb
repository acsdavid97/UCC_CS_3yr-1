{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Class, for use in pipelines, to select certain columns from a DataFrame and convert to a numpy array\n",
    "# From A. Geron: Hands-On Machine Learning with Scikit-Learn & TensorFlow, O'Reilly, 2017\n",
    "# Modified by Derek Bridge to allow for casting in the same ways as pandas.DataFrame.astype\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype=None):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_selected = X[self.attribute_names]\n",
    "        if self.dtype:\n",
    "            return X_selected.astype(self.dtype).values\n",
    "        return X_selected.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some functions to read the dataset from the files provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reads all files from the directory specified, and their content is returned as\n",
    "# a pandas Series of strings. \n",
    "def read_files_from_dir(directory):\n",
    "    files_contents = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path) as f:\n",
    "            files_contents.append(f.read())\n",
    "    return pd.Series(files_contents)\n",
    "\n",
    "# Converts a series to a dataframe and adds for each of the elements a \n",
    "# constant numeric label, specified in the parameter label. \n",
    "def to_pd_DF_with_label(ser, label):\n",
    "    df = pd.DataFrame()\n",
    "    df['text'] = ser\n",
    "    df['label'] = pd.Series(np.ones(len(df), dtype=np.int64) * label, index=df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the functions we defined, to actually read in the dataset.\n",
    "These lines of code assume that the spam and ham archives have been extracted to spam and ham directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read hams with label 0, since they are the negative class\n",
    "hams = to_pd_DF_with_label(read_files_from_dir('ham'), 0)\n",
    "# read spams with label 1, since they are the positive class\n",
    "spams = to_pd_DF_with_label(read_files_from_dir('spam'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1650, 2)\n",
      "(1248, 2)\n"
     ]
    }
   ],
   "source": [
    "# check if we succeeded in reading in the dataset.\n",
    "print(hams.shape)\n",
    "print(spams.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two separate dataframes, we should append one to the other, to have all data data in a single dataframe. After the append, we know that all hams are before all the spams, so we should shuffle the dataset to avoid problems with k-fold in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2898, 2)\n"
     ]
    }
   ],
   "source": [
    "emails = hams.append(spams, ignore_index=True)\n",
    "emails = emails.take(np.random.permutation(len(emails)))\n",
    "emails.reset_index(drop=True, inplace=True)\n",
    "print(emails.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the email files we can see that the first lines of all the emails are data about the email itself (metadata). Since we do not want to conduct metadata analysis of the email, we can delete this metadata, leaving us with the title and the body of the email. \n",
    "In order to strip the metadata we have to identify it. After opening a few files, I noticed a pattern: the metadata is delimited by an empty line in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emails['stripped_metadata'] = emails['text'].str.replace(r'(.*?)\\n\\n', '', flags=re.MULTILINE | re.DOTALL, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got rid of the metadata, the next thing I think to be unnecessary is the data found between HTML tags, so we could remove those too, in order to remain with only the plain text of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emails['just_text'] = emails['stripped_metadata'].str.replace(r\"<(.*?)>\", '', flags=re.MULTILINE | re.DOTALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run some tests later, I will strip the HTLM tags also, while leaving the metadata, so we can comapare these two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails['stripped_html'] = emails['text'].str.replace(r\"<(.*?)>\", '', flags=re.MULTILINE | re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessings = [('raw text', 'text'), \n",
    "                  ('stripped metadata', 'stripped_metadata'),\n",
    "                  ('stripped HTML tags', 'stripped_html'),\n",
    "                  ('stripped metadata and HTML', 'just_text'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect_eng_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "\n",
    "tfidf_eng_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "\n",
    "count_vect_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "\n",
    "dummy_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(['label'])),\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent')),\n",
    "])\n",
    "\n",
    "pipelines = [('Count vectorizer with English stop words', count_vect_eng_pipeline), \n",
    "             ('tf-idf vectorizer with English stop words', tfidf_eng_pipeline), \n",
    "             ('Count vectorizer', count_vect_pipeline), \n",
    "             ('tf-idf vectorizer', tfidf_pipeline), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the labels\n",
    "y = emails['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how the dummy classifier performs, which will predict the most frequent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56935926500417611"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(dummy_pipeline, emails, y, scoring='accuracy', cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the accuracy of the classifier, with 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97308197112516415"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(count_vect_pipeline, emails['stripped_metadata'], y, scoring='accuracy', cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1601,   49],\n",
       "       [  29, 1219]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = cross_val_predict(count_vect_pipeline, emails['stripped_metadata'], y, cv=10)\n",
    "confusion_matrix(y, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can observe, the classifier is not making too many false positives (ham classified as spam), the type of error we are trying to avoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer with English stop words raw text 0.9799856819\n",
      "Count vectorizer with English stop words stripped metadata 0.973081971125\n",
      "Count vectorizer with English stop words stripped HTML tags 0.981017778308\n",
      "Count vectorizer with English stop words stripped metadata and HTML 0.972728791314\n",
      "tf-idf vectorizer with English stop words raw text 0.954103328958\n",
      "tf-idf vectorizer with English stop words stripped metadata 0.9630783916\n",
      "tf-idf vectorizer with English stop words stripped HTML tags 0.959966591099\n",
      "tf-idf vectorizer with English stop words stripped metadata and HTML 0.97031499821\n",
      "Count vectorizer raw text 0.980330509486\n",
      "Count vectorizer stripped metadata 0.976529053812\n",
      "Count vectorizer stripped HTML tags 0.981706240305\n",
      "Count vectorizer stripped metadata and HTML 0.976872688223\n",
      "tf-idf vectorizer raw text 0.955137811717\n",
      "tf-idf vectorizer stripped metadata 0.967218708985\n",
      "tf-idf vectorizer stripped HTML tags 0.962728791314\n",
      "tf-idf vectorizer stripped metadata and HTML 0.971699081255\n"
     ]
    }
   ],
   "source": [
    "for pipeline_name, pipeline in pipelines:\n",
    "    for preproc_name, preproc in preprocessings:\n",
    "        mean = np.mean(cross_val_score(pipeline, emails[preproc], y, cv=10))\n",
    "        print(pipeline_name, preproc_name, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n \\n        \\n        \\n\\nZDNet AnchorDesk Newsletter\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n      \\n\\n\\n\\n\\n\\n-->\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n                \\n                        \\n                        \\n                        \\n                        \\n                        \\n                                TUESDAY, JULY 16, 2002                                                        \\n                        \\n                        \\n                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \\n      \\n         \\n             \\n         \\n      \\n   \\n\\n\\n      \\n          \\n             \\n                &nbsp;David Coursey\\n             \\n          \\n          \\n             \\n          \\n      \\n   \\n\\n   \\n      \\n         \\n            \\n         \\n      \\n      \\n         \\n            \\n         \\n      \\n   \\n-->\\n\\n   \\n   \\n   Why we\\'re changing our publishing schedule\\n\\n\\n\\nDear Reader,\\n\\nAs of Monday, July 15, AnchorDesk is being published on Mondays, Wednesdays, and Fridays, instead of five days a week. The AnchorDesk Weekly newsletter will continued to be published every Friday\\n\\nWhy are we doing this? There are several reasons, but the most important one is giving me a chance do more than just sit behind a desk typing all day in order to meet deadlines.  I\\'ve been writing five columns a week for 18 months now, and I\\'d be lying to you if I didn\\'t admit to being more than a little tired.  \\n\\nAlso, I\\'ve been on such a short leash that I haven\\'t been able to travel or meet as many people as I\\'d like. This new schedule means I\\'ll have more time to get out, learn more, see more products, and discover new issues or trends. So it should result in a more interesting and useful AnchorDesk for you.\\n\\nHaving additional time also means I will be able to work on several projects I\\'ve been assigned, including one that could bear fruit at the Consumer Electronics Show next January. More about that soon, once all the details are nailed down.\\n\\nFor now, the new schedule is being called a \"test,\" which means it could turn out to be temporary--or maybe not. We\\'ll run it through the end of the summer and see how it works. \\n\\nI appreciate your continued support of AnchorDesk. While the column will be less frequent, I will still be on CNET Radio every weekday at noon PT (or on-demand streaming anytime) and hope you will join me there.\\n\\nSincerely,\\n\\n\\nDavid Coursey\\nExecutive Editor\\nAnchorDesk\\n\\n\\n   \\n   \\n\\n\\t\\n   \\n   \\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n        \\n        \\n        \\n        \\n \\n        \\n        \\n        \\n\\n         \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up for more free newsletters from ZDNet\\n\\n\\n\\n\\nThe e-mail address for your subscription is&nbsp;qqqqqqqqqq-zdnet@spamassassin.taint.org\\n       \\n        Unsubscribe&nbsp;| \\n        &nbsp;Manage \\n        My Subscriptions&nbsp;|&nbsp;FAQ&nbsp;| \\n        &nbsp;Advertise\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome |eBusiness | Security | Networking | Applications | Platforms | Hardware | Careers\\n\\n\\n\\n\\n        Copyright 2002 CNET Networks, Inc. All rights reserved. ZDNet is a registered service mark of CNET Networks, Inc.                 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(emails['just_text'])[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
