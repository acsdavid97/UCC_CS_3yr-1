{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Class, for use in pipelines, to select certain columns from a DataFrame and convert to a numpy array\n",
    "# From A. Geron: Hands-On Machine Learning with Scikit-Learn & TensorFlow, O'Reilly, 2017\n",
    "# Modified by Derek Bridge to allow for casting in the same ways as pandas.DataFrame.astype\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype=None):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_selected = X[self.attribute_names]\n",
    "        if self.dtype:\n",
    "            return X_selected.astype(self.dtype).values\n",
    "        return X_selected.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the archives, we can see that there are two kinds emails (spam and ham, obviously) in different folders.\n",
    "So we can identify the label of an example by looking at the directory it is stored in.\n",
    "\n",
    "Opening some of the emails, we can observe that each one of them is starting with some metadata about the email itself, and the content of the email is following the metadata. Some HTML tags are present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some functions to read the dataset from the files provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reads all files from the directory specified, and their content is returned as\n",
    "# a pandas Series of strings. \n",
    "def read_files_from_dir(directory):\n",
    "    files_contents = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path) as f:\n",
    "            files_contents.append(f.read())\n",
    "    return pd.Series(files_contents)\n",
    "\n",
    "# Converts a series to a dataframe and adds for each of the elements a \n",
    "# constant numeric label, specified in the parameter label. \n",
    "def to_pd_DF_with_label(ser, label):\n",
    "    df = pd.DataFrame()\n",
    "    df['text'] = ser\n",
    "    df['label'] = pd.Series(np.ones(len(df), dtype=np.int64) * label, index=df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the functions we defined, to actually read in the dataset.\n",
    "These lines of code assume that the spam and ham archives \n",
    "have been extracted to directories spam and ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read hams with label 0, since they are the negative class\n",
    "hams = to_pd_DF_with_label(read_files_from_dir('ham'), 0)\n",
    "# read spams with label 1, since they are the positive class\n",
    "spams = to_pd_DF_with_label(read_files_from_dir('spam'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1650, 2)\n",
      "(1248, 2)\n"
     ]
    }
   ],
   "source": [
    "# check if we succeeded in reading in the dataset.\n",
    "print(hams.shape)\n",
    "print(spams.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two separate dataframes, we should append one to the other,\n",
    "to have all data data in a single dataframe.\n",
    "After the append, we know that all hams are before all the spams,\n",
    "so we should shuffle the dataset to avoid problems with k-fold in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2898, 2)\n"
     ]
    }
   ],
   "source": [
    "emails = hams.append(spams, ignore_index=True)\n",
    "emails = emails.take(np.random.permutation(len(emails)))\n",
    "emails.reset_index(drop=True, inplace=True)\n",
    "print(emails.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the email files, we can see that the first lines of all the emails \n",
    "are data about the email itself (metadata).\n",
    "Since we do not want to conduct metadata analysis of the email,\n",
    "we can delete this metadata, leaving us with the title and the body of the email. \n",
    "In order to strip the metadata we have to identify it.\n",
    "After opening a few files, I noticed a pattern:\n",
    "the metadata is delimited by an empty line in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emails['stripped_metadata'] = emails['text'].str.replace(r'(.*?)\\n\\n', '', flags=re.MULTILINE | re.DOTALL, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got rid of the metadata, the next thing I think to be unnecessary\n",
    "is the data found between HTML tags, so we could remove those too,\n",
    "in order to remain with only the plain text of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emails['just_text'] = emails['stripped_metadata'].str.replace(r\"<(.*?)>\", '', flags=re.MULTILINE | re.DOTALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run some tests later, I will strip the HTLM tags also,\n",
    "while leaving the metadata, so we can comapare these two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emails['stripped_html'] = emails['text'].str.replace(r\"<(.*?)>\", '', flags=re.MULTILINE | re.DOTALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All types of preprocessing are stored in this list of tuples,\n",
    "where the first element of the tuple represents name of the preprocessing type\n",
    "and the second element of the tuple represents the corresponding column from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessings = [('raw text', 'text'), \n",
    "                  ('stripped metadata', 'stripped_metadata'),\n",
    "                  ('stripped HTML tags', 'stripped_html'),\n",
    "                  ('stripped metadata and HTML', 'just_text'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, we will build the pipelines,\n",
    "which will be responsible for doing the vectorization of the emails and their classification.\n",
    "Some of them are really similar, with diferent paramters or diffrent steps in the pipeline.\n",
    "Please note: I did not present here all the possibilities that I tried,\n",
    "since the results are really similiar, and there is no need to replicate so much of the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some paramters to count vectorizer: minimum document frequency should be 0.01 and max 0.5\n",
    "count_vect_eng_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english', min_df=0.01, max_df=0.5)),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# some paramters to tf-idf vectorizer: minimum document frequency should be 0.01 and max 0.5\n",
    "tfidf_eng_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', min_df=0.01, max_df=0.5)),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# all default CountVectorizer + LogisticRegression\n",
    "count_vect_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# all default TfidfVectorizer + LogisticRegression\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# For SGD I am using the hinge loss function,\n",
    "# if the log function would be used, we would get Logistic Regression\n",
    "\n",
    "# all default CountVectorizer + SGDClassifier\n",
    "count_vect_pipeline_sgd = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', SGDClassifier(max_iter=1000, loss='hinge')),\n",
    "])\n",
    "\n",
    "# all default TfidfVectorizer + SGDClassifier, using hinge loss function\n",
    "\n",
    "tfidf_pipeline_sgd = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', SGDClassifier(max_iter=1000, loss='hinge')),\n",
    "])\n",
    "\n",
    "# just a dummy pipeline, which will always predict the mode.\n",
    "dummy_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(['label'])),\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent')),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same idea is used as when constructing the preprocessings list: \n",
    "list of tuples, first name, second pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [('Count vectorizer with English stop words', count_vect_eng_pipeline), \n",
    "             ('tf-idf vectorizer with English stop words', tfidf_eng_pipeline), \n",
    "             ('Count vectorizer', count_vect_pipeline), \n",
    "             ('tf-idf vectorizer', tfidf_pipeline), \n",
    "             ('Count vectorizer + SGD', count_vect_pipeline_sgd),\n",
    "             ('tf-idf vectorizer + SGD', tfidf_pipeline_sgd),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start estimating the accuracy of the pipelines, \n",
    "we should talk about the accuracy measures we use in order to evaluate the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose stratified k-fold as my accuracy measure, with k=10,\n",
    "because we have a lot of examples, so it will perform better than Holdout.\n",
    "In each fold we will have more than 289 example. \n",
    "Having so many examples in each fold ensures that the measurement is \n",
    "statistically significant (we should have at least 30 examples in each fold).\n",
    "\n",
    "Stratified k-fold is better than holdout (in this case), due to the fact that,\n",
    "k-fold will perform the stratification and testing multiple times, \n",
    "so the chances of getting 'unlucky' will be less, compared to using simple holdout. \n",
    "\n",
    "When using k-fold, it is important to shuffle the dataset, \n",
    "since if the dataset is sorted, in each fold a particular type \n",
    "of examples may be included, which will result in a skewed result.\n",
    "\n",
    "The shuffling of the dataset has been done previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the labels of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the labels\n",
    "y = emails['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how the dummy classifier performs, which will predict the most frequent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56935926500417611"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(dummy_pipeline, emails, y, scoring='accuracy', cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the accuracy of a classifier, with 10-fold cross validation, and stripped metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97618541940102599"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(count_vect_pipeline, emails['stripped_metadata'], y, scoring='accuracy', cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1613,   37],\n",
       "       [  32, 1216]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = cross_val_predict(count_vect_pipeline, emails['stripped_metadata'], y, cv=10)\n",
    "confusion_matrix(y, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can observe, the classifier is not making too many false positives \n",
    "(ham classified as spam), the type of error we are trying to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual testing is nice and all, but we have too many possibilities to check we should automate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the estimation for all pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the above presented pipelines and with each of the possible preprocessing steps, I wrote two simple fors, which will check all possible combinations of these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer with English stop words raw text 0.9799856819\n",
      "Count vectorizer with English stop words stripped metadata 0.97445889512\n",
      "Count vectorizer with English stop words stripped HTML tags 0.980329316311\n",
      "Count vectorizer with English stop words stripped metadata and HTML 0.97445173607\n",
      "tf-idf vectorizer with English stop words raw text 0.955146163942\n",
      "tf-idf vectorizer with English stop words stripped metadata 0.959621763513\n",
      "tf-idf vectorizer with English stop words stripped HTML tags 0.961351867319\n",
      "tf-idf vectorizer with English stop words stripped metadata and HTML 0.966174680826\n",
      "Count vectorizer raw text 0.980674143897\n",
      "Count vectorizer stripped metadata 0.976185419401\n",
      "Count vectorizer stripped HTML tags 0.982398281828\n",
      "Count vectorizer stripped metadata and HTML 0.976527860637\n",
      "tf-idf vectorizer raw text 0.956179453526\n",
      "tf-idf vectorizer stripped metadata 0.966525474287\n",
      "tf-idf vectorizer stripped HTML tags 0.96308077795\n",
      "tf-idf vectorizer stripped metadata and HTML 0.972043908841\n",
      "Count vectorizer + SGD raw text 0.976181839876\n",
      "Count vectorizer + SGD stripped metadata 0.969288867677\n",
      "Count vectorizer + SGD stripped HTML tags 0.975495764229\n",
      "Count vectorizer + SGD stripped metadata and HTML 0.964798950006\n",
      "tf-idf vectorizer + SGD raw text 0.981708626656\n",
      "tf-idf vectorizer + SGD stripped metadata 0.982399475003\n",
      "tf-idf vectorizer + SGD stripped HTML tags 0.985502923279\n",
      "tf-idf vectorizer + SGD stripped metadata and HTML 0.982048681542\n"
     ]
    }
   ],
   "source": [
    "for pipeline_name, pipeline in pipelines:\n",
    "    for preproc_name, preproc in preprocessings:\n",
    "        mean = np.mean(cross_val_score(pipeline, emails[preproc], y, cv=10))\n",
    "        print(pipeline_name, preproc_name, mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Intrestingly, the tf-idf vectorizer, performed generally poorer \n",
    "than the CountVectorizer. The CountVectorizer seems to lose accuracy \n",
    "as the data is preprocessed, the tf-idf approach is gaining accuracy if the data is preprocessed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also it looks like discarding the English stopwords has little effects, \n",
    "but the pipelines without the discarding of stopwords are performing slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best method, according to the accuracy is the CountVectorizer \n",
    "with the stippped HTML tags and not discarding the English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer with English stop words raw text\n",
      "[[1621   29]\n",
      " [  29 1219]]\n",
      "Count vectorizer with English stop words stripped metadata\n",
      "[[1608   42]\n",
      " [  32 1216]]\n",
      "Count vectorizer with English stop words stripped HTML tags\n",
      "[[1617   33]\n",
      " [  24 1224]]\n",
      "Count vectorizer with English stop words stripped metadata and HTML\n",
      "[[1608   42]\n",
      " [  32 1216]]\n",
      "tf-idf vectorizer with English stop words raw text\n",
      "[[1589   61]\n",
      " [  69 1179]]\n",
      "tf-idf vectorizer with English stop words stripped metadata\n",
      "[[1585   65]\n",
      " [  52 1196]]\n",
      "tf-idf vectorizer with English stop words stripped HTML tags\n",
      "[[1605   45]\n",
      " [  67 1181]]\n",
      "tf-idf vectorizer with English stop words stripped metadata and HTML\n",
      "[[1594   56]\n",
      " [  42 1206]]\n",
      "Count vectorizer raw text\n",
      "[[1620   30]\n",
      " [  26 1222]]\n",
      "Count vectorizer stripped metadata\n",
      "[[1613   37]\n",
      " [  32 1216]]\n",
      "Count vectorizer stripped HTML tags\n",
      "[[1622   28]\n",
      " [  23 1225]]\n",
      "Count vectorizer stripped metadata and HTML\n",
      "[[1612   38]\n",
      " [  30 1218]]\n",
      "tf-idf vectorizer raw text\n",
      "[[1593   57]\n",
      " [  70 1178]]\n",
      "tf-idf vectorizer stripped metadata\n",
      "[[1602   48]\n",
      " [  49 1199]]\n",
      "tf-idf vectorizer stripped HTML tags\n",
      "[[1606   44]\n",
      " [  63 1185]]\n",
      "tf-idf vectorizer stripped metadata and HTML\n",
      "[[1605   45]\n",
      " [  36 1212]]\n",
      "Count vectorizer + SGD raw text\n",
      "[[1617   33]\n",
      " [  41 1207]]\n",
      "Count vectorizer + SGD stripped metadata\n",
      "[[1613   37]\n",
      " [  51 1197]]\n",
      "Count vectorizer + SGD stripped HTML tags\n",
      "[[1610   40]\n",
      " [  39 1209]]\n",
      "Count vectorizer + SGD stripped metadata and HTML\n",
      "[[1599   51]\n",
      " [  46 1202]]\n",
      "tf-idf vectorizer + SGD raw text\n",
      "[[1626   24]\n",
      " [  29 1219]]\n",
      "tf-idf vectorizer + SGD stripped metadata\n",
      "[[1623   27]\n",
      " [  24 1224]]\n",
      "tf-idf vectorizer + SGD stripped HTML tags\n",
      "[[1632   18]\n",
      " [  24 1224]]\n",
      "tf-idf vectorizer + SGD stripped metadata and HTML\n",
      "[[1623   27]\n",
      " [  25 1223]]\n"
     ]
    }
   ],
   "source": [
    "for pipeline_name, pipeline in pipelines:\n",
    "    for preproc_name, preproc in preprocessings:\n",
    "        y_predicted = cross_val_predict(pipeline, emails[preproc], y, cv=10)\n",
    "        conf_matrix = confusion_matrix(y, y_predicted)\n",
    "        print(pipeline_name, preproc_name)\n",
    "        print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusion matrices we arrive at the same conclusion, \n",
    "the CountVectorizer without the stopwords and with stripped HTML tags seems to be the most promising choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
